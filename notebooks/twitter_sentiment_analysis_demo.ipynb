{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daf9fad9",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis v·ªõi RoBERTa Model\n",
    "\n",
    "## T√°i hi·ªán ƒë·ªì √°n NLP ph√¢n t√≠ch c·∫£m x√∫c v·ªõi ki·∫øn tr√∫c ƒë∆°n gi·∫£n h√≥a\n",
    "\n",
    "**D·ª± √°n n√†y t√°i hi·ªán v√† m·ªü r·ªông nghi√™n c·ª©u v·ªÅ ph√¢n t√≠ch c·∫£m x√∫c Twitter s·ª≠ d·ª•ng m√¥ h√¨nh RoBERTa-Twitter, d·ª±a tr√™n ƒë·ªì √°n g·ªëc c·ªßa anh Th·ªãnh L√¢m T·∫•n.**\n",
    "\n",
    "### üéØ **M·ª•c ti√™u**\n",
    "- T√°i hi·ªán m√¥ h√¨nh AI ph√¢n t√≠ch c·∫£m x√∫c v·ªõi d·ªØ li·ªáu m·ªõi\n",
    "- S·ª≠ d·ª•ng RoBERTa-Twitter model t·ª´ Hugging Face\n",
    "- ƒê∆°n gi·∫£n h√≥a ki·∫øn tr√∫c: Python + Pandas + Transformers (thay v√¨ Kafka + Spark + MongoDB)\n",
    "- T·∫°o tr·ª±c quan h√≥a t∆∞∆°ng t·ª± ƒë·ªì √°n g·ªëc\n",
    "\n",
    "### üîÑ **Quy tr√¨nh**\n",
    "```\n",
    "Crawl Data ‚Üí Text Preprocessing ‚Üí RoBERTa Analysis ‚Üí Visualization ‚Üí Report\n",
    "```\n",
    "\n",
    "### üìä **So s√°nh ki·∫øn tr√∫c**\n",
    "| ƒê·ªì √°n g·ªëc | D·ª± √°n hi·ªán t·∫°i |\n",
    "|-----------|----------------|\n",
    "| Producer ‚Üí Kafka ‚Üí Spark ‚Üí MongoDB | Python Script ‚Üí CSV ‚Üí RoBERTa ‚Üí Charts |\n",
    "| Big Data Architecture | Simplified Pipeline |\n",
    "| Distributed Processing | Single Machine |\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook n√†y s·∫Ω h∆∞·ªõng d·∫´n chi ti·∫øt t·ª´ng b∆∞·ªõc ƒë·ªÉ x√¢y d·ª±ng h·ªá th·ªëng ph√¢n t√≠ch c·∫£m x√∫c ho√†n ch·ªânh.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89be4a5",
   "metadata": {},
   "source": [
    "# 1. Thi·∫øt l·∫≠p M√¥i tr∆∞·ªùng v√† C√†i ƒë·∫∑t Th∆∞ vi·ªán\n",
    "\n",
    "ƒê·∫ßu ti√™n, ch√∫ng ta s·∫Ω c√†i ƒë·∫∑t v√† import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt cho d·ª± √°n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f54babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt (ch·∫°y l·∫ßn ƒë·∫ßu)\n",
    "# !pip install pandas requests transformers torch matplotlib seaborn plotly tqdm python-dotenv\n",
    "\n",
    "# Import c√°c th∆∞ vi·ªán ch√≠nh\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Any\n",
    "\n",
    "# Th∆∞ vi·ªán cho machine learning v√† NLP\n",
    "try:\n",
    "    from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "    import torch\n",
    "    print(\"‚úÖ Transformers v√† PyTorch ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "except ImportError as e:\n",
    "    print(\"‚ùå L·ªói import transformers/torch:\", e)\n",
    "    print(\"Vui l√≤ng ch·∫°y: !pip install transformers torch\")\n",
    "\n",
    "# Th∆∞ vi·ªán cho visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Thi·∫øt l·∫≠p style cho plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"üöÄ THI·∫æT L·∫¨P HO√ÄN T·∫§T\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c979e2d9",
   "metadata": {},
   "source": [
    "# 2. Thu th·∫≠p D·ªØ li·ªáu t·ª´ Twitter API\n",
    "\n",
    "Ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng twitterapi.io ƒë·ªÉ thu th·∫≠p tweets v·ªõi c√°c t·ª´ kh√≥a AI ph·ªï bi·∫øn, t∆∞∆°ng t·ª± nh∆∞ ƒë·ªì √°n g·ªëc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beceab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C·∫•u h√¨nh API v√† t·ª´ kh√≥a (gi·ªëng ƒë·ªì √°n g·ªëc + th√™m t·ª´ kh√≥a m·ªõi)\n",
    "TWITTER_API_KEY = \"your_api_key_here\"  # Thay th·∫ø b·∫±ng API key th·ª±c t·ª´ twitterapi.io\n",
    "TWITTER_API_BASE_URL = \"https://api.twitterapi.io/v1\"\n",
    "\n",
    "# T·ª´ kh√≥a t·ª´ ƒë·ªì √°n g·ªëc + t·ª´ kh√≥a m·ªõi\n",
    "KEYWORDS = [\n",
    "    \"GPT\", \"Copilot\", \"Gemini\",      # T·ª´ ƒë·ªì √°n g·ªëc c·ªßa anh Th·ªãnh\n",
    "    \"GPT-4o\", \"Sora\", \"Llama 3\",     # T·ª´ kh√≥a AI m·ªõi\n",
    "    \"Claude\", \"ChatGPT\"              # B·ªï sung th√™m\n",
    "]\n",
    "\n",
    "MAX_TWEETS_PER_KEYWORD = 100  # Gi·∫£m ƒë·ªÉ demo nhanh\n",
    "RATE_LIMIT_DELAY = 2\n",
    "\n",
    "print(\"üìã C·∫•u h√¨nh thu th·∫≠p d·ªØ li·ªáu:\")\n",
    "print(f\"   Keywords: {KEYWORDS}\")\n",
    "print(f\"   Max tweets per keyword: {MAX_TWEETS_PER_KEYWORD}\")\n",
    "print(f\"   Total expected tweets: {len(KEYWORDS) * MAX_TWEETS_PER_KEYWORD}\")\n",
    "\n",
    "class TwitterCrawler:\n",
    "    \"\"\"\n",
    "    Twitter data crawler s·ª≠ d·ª•ng twitterapi.io\n",
    "    T∆∞∆°ng t·ª± ƒë·ªì √°n g·ªëc nh∆∞ng ƒë∆°n gi·∫£n h√≥a\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = TWITTER_API_BASE_URL\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'Authorization': f'Bearer {api_key}',\n",
    "            'Content-Type': 'application/json'\n",
    "        })\n",
    "    \n",
    "    def search_tweets(self, query: str, max_results: int = 100) -> List[Dict]:\n",
    "        \"\"\"Thu th·∫≠p tweets theo t·ª´ kh√≥a\"\"\"\n",
    "        endpoint = f\"{self.base_url}/search\"\n",
    "        \n",
    "        params = {\n",
    "            'query': query,\n",
    "            'max_results': min(max_results, 100),\n",
    "            'tweet.fields': 'created_at,author_id,public_metrics,lang',\n",
    "            'user.fields': 'name,username,verified',\n",
    "            'expansions': 'author_id'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(endpoint, params=params)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            tweets = []\n",
    "            \n",
    "            if 'data' in data:\n",
    "                # T·∫°o mapping user info\n",
    "                users = {}\n",
    "                if 'includes' in data and 'users' in data['includes']:\n",
    "                    users = {user['id']: user for user in data['includes']['users']}\n",
    "                \n",
    "                for tweet in data['data']:\n",
    "                    user_info = users.get(tweet.get('author_id', ''), {})\n",
    "                    \n",
    "                    tweet_data = {\n",
    "                        'id': tweet.get('id', ''),\n",
    "                        'text': tweet.get('text', ''),\n",
    "                        'created_at': tweet.get('created_at', ''),\n",
    "                        'username': user_info.get('username', ''),\n",
    "                        'user_name': user_info.get('name', ''),\n",
    "                        'retweet_count': tweet.get('public_metrics', {}).get('retweet_count', 0),\n",
    "                        'like_count': tweet.get('public_metrics', {}).get('like_count', 0),\n",
    "                        'lang': tweet.get('lang', ''),\n",
    "                        'keyword': query\n",
    "                    }\n",
    "                    tweets.append(tweet_data)\n",
    "            \n",
    "            return tweets\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ùå L·ªói API cho t·ª´ kh√≥a '{query}': {e}\")\n",
    "            return []\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"‚ùå L·ªói parse JSON cho t·ª´ kh√≥a '{query}': {e}\")\n",
    "            return []\n",
    "    \n",
    "    def crawl_multiple_keywords(self, keywords: List[str], max_tweets: int) -> pd.DataFrame:\n",
    "        \"\"\"Thu th·∫≠p tweets cho nhi·ªÅu t·ª´ kh√≥a\"\"\"\n",
    "        all_tweets = []\n",
    "        \n",
    "        print(f\"üîÑ B·∫Øt ƒë·∫ßu thu th·∫≠p tweets cho {len(keywords)} t·ª´ kh√≥a...\")\n",
    "        \n",
    "        for i, keyword in enumerate(keywords, 1):\n",
    "            print(f\"   [{i}/{len(keywords)}] ƒêang crawl: '{keyword}'\")\n",
    "            \n",
    "            tweets = self.search_tweets(keyword, max_tweets)\n",
    "            \n",
    "            if tweets:\n",
    "                all_tweets.extend(tweets)\n",
    "                print(f\"      ‚úÖ Thu th·∫≠p ƒë∆∞·ª£c {len(tweets)} tweets\")\n",
    "            else:\n",
    "                print(f\"      ‚ö†Ô∏è Kh√¥ng thu th·∫≠p ƒë∆∞·ª£c tweets n√†o\")\n",
    "            \n",
    "            # Rate limiting\n",
    "            if i < len(keywords):  # Kh√¥ng delay ·ªü l·∫ßn cu·ªëi\n",
    "                time.sleep(RATE_LIMIT_DELAY)\n",
    "        \n",
    "        # Chuy·ªÉn th√†nh DataFrame\n",
    "        df = pd.DataFrame(all_tweets)\n",
    "        \n",
    "        if not df.empty:\n",
    "            # X·ª≠ l√Ω d·ªØ li·ªáu\n",
    "            df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "            df = df.drop_duplicates(subset=['id'])  # Lo·∫°i b·ªè duplicates\n",
    "            df = df[df['lang'] == 'en']  # Ch·ªâ l·∫•y tweets ti·∫øng Anh\n",
    "            \n",
    "            print(f\"‚úÖ Ho√†n th√†nh! T·ªïng c·ªông {len(df)} tweets unique (ti·∫øng Anh)\")\n",
    "        else:\n",
    "            print(\"‚ùå Kh√¥ng thu th·∫≠p ƒë∆∞·ª£c d·ªØ li·ªáu n√†o\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# T·∫°o crawler instance \n",
    "print(\"ü§ñ Kh·ªüi t·∫°o Twitter Crawler...\")\n",
    "if TWITTER_API_KEY == \"your_api_key_here\":\n",
    "    print(\"‚ö†Ô∏è C·∫¢NH B√ÅO: B·∫°n c·∫ßn thay th·∫ø TWITTER_API_KEY b·∫±ng API key th·ª±c\")\n",
    "    print(\"   ƒê·ªÉ demo, ch√∫ng ta s·∫Ω t·∫°o d·ªØ li·ªáu m·∫´u...\")\n",
    "    use_real_api = False\n",
    "else:\n",
    "    crawler = TwitterCrawler(TWITTER_API_KEY)\n",
    "    use_real_api = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f104fdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thu th·∫≠p d·ªØ li·ªáu th·ª±c t·∫ø ho·∫∑c t·∫°o d·ªØ li·ªáu m·∫´u ƒë·ªÉ demo\n",
    "if use_real_api:\n",
    "    # S·ª≠ d·ª•ng API th·ª±c\n",
    "    print(\"üîÑ Thu th·∫≠p d·ªØ li·ªáu t·ª´ Twitter API...\")\n",
    "    raw_data = crawler.crawl_multiple_keywords(KEYWORDS, MAX_TWEETS_PER_KEYWORD)\n",
    "else:\n",
    "    # T·∫°o d·ªØ li·ªáu m·∫´u ƒë·ªÉ demo\n",
    "    print(\"üîß T·∫°o d·ªØ li·ªáu m·∫´u ƒë·ªÉ demo...\")\n",
    "    \n",
    "    sample_tweets = [\n",
    "        \"I love using GPT-4! It's amazing for coding assistance and problem solving.\",\n",
    "        \"ChatGPT is helpful but sometimes gives wrong information. Need to be careful.\",\n",
    "        \"GitHub Copilot saves me so much time when programming. Highly recommend!\",\n",
    "        \"Gemini is decent but I still prefer GPT for most tasks.\",\n",
    "        \"The new GPT-4o model is incredibly fast and accurate. Impressed!\",\n",
    "        \"Sora AI video generation is mind-blowing. The future is here!\",\n",
    "        \"Llama 3 open source model is surprisingly good for a free alternative.\",\n",
    "        \"Claude is great for writing and analysis tasks. Very thoughtful responses.\",\n",
    "        \"AI copilots in coding are game changers. Can't imagine coding without them now.\",\n",
    "        \"These AI tools are making everyone more productive. Exciting times!\",\n",
    "        \"GPT sometimes hallucinates facts. Always double-check important information.\",\n",
    "        \"Copilot suggestions are usually good but sometimes completely off-topic.\",\n",
    "        \"I'm worried about AI replacing human creativity and jobs.\",\n",
    "        \"The quality of AI responses keeps getting better every month.\",\n",
    "        \"Using multiple AI tools together gives the best results for complex tasks.\"\n",
    "    ] * 10  # Repeat ƒë·ªÉ c√≥ ƒë·ªß d·ªØ li·ªáu\n",
    "    \n",
    "    # T·∫°o DataFrame m·∫´u\n",
    "    import random\n",
    "    \n",
    "    raw_data = []\n",
    "    for i, text in enumerate(sample_tweets):\n",
    "        tweet_data = {\n",
    "            'id': f'tweet_{i}',\n",
    "            'text': text,\n",
    "            'created_at': pd.Timestamp.now() - pd.Timedelta(days=random.randint(0, 30)),\n",
    "            'username': f'user_{i % 20}',\n",
    "            'user_name': f'User {i % 20}',\n",
    "            'retweet_count': random.randint(0, 100),\n",
    "            'like_count': random.randint(0, 500),\n",
    "            'lang': 'en',\n",
    "            'keyword': random.choice(KEYWORDS)\n",
    "        }\n",
    "        raw_data.append(tweet_data)\n",
    "    \n",
    "    raw_data = pd.DataFrame(raw_data)\n",
    "    print(f\"‚úÖ T·∫°o th√†nh c√¥ng {len(raw_data)} tweets m·∫´u\")\n",
    "\n",
    "# Hi·ªÉn th·ªã th√¥ng tin d·ªØ li·ªáu thu th·∫≠p ƒë∆∞·ª£c\n",
    "print(\"\\\\nüìä TH√îNG TIN D·ªÆ LI·ªÜU THU TH·∫¨P:\")\n",
    "print(f\"   T·ªïng s·ªë tweets: {len(raw_data)}\")\n",
    "print(f\"   Kho·∫£ng th·ªùi gian: {raw_data['created_at'].min()} ƒë·∫øn {raw_data['created_at'].max()}\")\n",
    "\n",
    "# Ph√¢n b·ªë theo t·ª´ kh√≥a\n",
    "keyword_distribution = raw_data['keyword'].value_counts()\n",
    "print(\"\\\\nüìà Ph√¢n b·ªë theo t·ª´ kh√≥a:\")\n",
    "for keyword, count in keyword_distribution.items():\n",
    "    print(f\"   {keyword}: {count} tweets\")\n",
    "\n",
    "# Hi·ªÉn th·ªã m·∫´u d·ªØ li·ªáu\n",
    "print(\"\\\\nüîç M·∫´u d·ªØ li·ªáu:\")\n",
    "print(raw_data[['text', 'username', 'keyword', 'created_at']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d0b42",
   "metadata": {},
   "source": [
    "# 3. Ti·ªÅn x·ª≠ l√Ω v√† L√†m s·∫°ch VƒÉn b·∫£n\n",
    "\n",
    "√Åp d·ª•ng c√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω gi·ªëng nh∆∞ ƒë·ªì √°n g·ªëc: chuy·ªÉn ch·ªØ th∆∞·ªùng, lo·∫°i b·ªè URL, mentions, hashtags, k√Ω t·ª± ƒë·∫∑c bi·ªát."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c26b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Text preprocessing class theo ƒë·ªì √°n g·ªëc\n",
    "    √Åp d·ª•ng c√°c b∆∞·ªõc l√†m s·∫°ch t∆∞∆°ng t·ª± SQLTransformer trong Spark\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cleaning_stats = {\n",
    "            'urls_removed': 0,\n",
    "            'mentions_removed': 0,\n",
    "            'hashtags_removed': 0,\n",
    "            'texts_too_short': 0\n",
    "        }\n",
    "    \n",
    "    def remove_urls(self, text: str) -> str:\n",
    "        \"\"\"Lo·∫°i b·ªè URLs\"\"\"\n",
    "        url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        if url_pattern.search(text):\n",
    "            self.cleaning_stats['urls_removed'] += 1\n",
    "        return url_pattern.sub('', text)\n",
    "    \n",
    "    def remove_mentions(self, text: str) -> str:\n",
    "        \"\"\"Lo·∫°i b·ªè @mentions\"\"\"\n",
    "        mention_pattern = re.compile(r'@[\\\\w_]+')\n",
    "        if mention_pattern.search(text):\n",
    "            self.cleaning_stats['mentions_removed'] += 1\n",
    "        return mention_pattern.sub('', text)\n",
    "    \n",
    "    def remove_hashtags(self, text: str) -> str:\n",
    "        \"\"\"Lo·∫°i b·ªè hashtags\"\"\"\n",
    "        hashtag_pattern = re.compile(r'#[\\\\w_]+')\n",
    "        if hashtag_pattern.search(text):\n",
    "            self.cleaning_stats['hashtags_removed'] += 1\n",
    "        return hashtag_pattern.sub('', text)\n",
    "    \n",
    "    def remove_special_chars(self, text: str) -> str:\n",
    "        \"\"\"Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát, ch·ªâ gi·ªØ ch·ªØ c√°i v√† s·ªë\"\"\"\n",
    "        return re.sub(r'[^a-zA-Z0-9\\\\s]', '', text)\n",
    "    \n",
    "    def remove_extra_whitespace(self, text: str) -> str:\n",
    "        \"\"\"Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a\"\"\"\n",
    "        return re.sub(r'\\\\s+', ' ', text).strip()\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        √Åp d·ª•ng t·∫•t c·∫£ c√°c b∆∞·ªõc l√†m s·∫°ch\n",
    "        Theo th·ª© t·ª± gi·ªëng ƒë·ªì √°n g·ªëc\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return \"\"\n",
    "        \n",
    "        # 1. Chuy·ªÉn ch·ªØ th∆∞·ªùng (gi·ªëng ƒë·ªì √°n g·ªëc)\n",
    "        text = text.lower()\n",
    "        \n",
    "        # 2. Lo·∫°i b·ªè URLs\n",
    "        text = self.remove_urls(text)\n",
    "        \n",
    "        # 3. Lo·∫°i b·ªè mentions v√† hashtags\n",
    "        text = self.remove_mentions(text)\n",
    "        text = self.remove_hashtags(text)\n",
    "        \n",
    "        # 4. Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát\n",
    "        text = self.remove_special_chars(text)\n",
    "        \n",
    "        # 5. Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a\n",
    "        text = self.remove_extra_whitespace(text)\n",
    "        \n",
    "        # 6. Ki·ªÉm tra ƒë·ªô d√†i t·ªëi thi·ªÉu\n",
    "        if len(text) < 10:\n",
    "            self.cleaning_stats['texts_too_short'] += 1\n",
    "            return \"\"\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def preprocess_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"X·ª≠ l√Ω to√†n b·ªô DataFrame\"\"\"\n",
    "        print(\"üßπ B·∫Øt ƒë·∫ßu ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n...\")\n",
    "        \n",
    "        # Reset stats\n",
    "        self.cleaning_stats = {key: 0 for key in self.cleaning_stats}\n",
    "        \n",
    "        # T·∫°o copy ƒë·ªÉ kh√¥ng thay ƒë·ªïi d·ªØ li·ªáu g·ªëc\n",
    "        processed_df = df.copy()\n",
    "        \n",
    "        # √Åp d·ª•ng l√†m s·∫°ch\n",
    "        processed_df['cleaned_text'] = processed_df['text'].apply(self.clean_text)\n",
    "        \n",
    "        # Lo·∫°i b·ªè tweets r·ªóng sau khi l√†m s·∫°ch\n",
    "        original_count = len(processed_df)\n",
    "        processed_df = processed_df[processed_df['cleaned_text'].str.len() > 0]\n",
    "        removed_count = original_count - len(processed_df)\n",
    "        \n",
    "        # Th√™m metadata\n",
    "        processed_df['original_length'] = df['text'].str.len()\n",
    "        processed_df['cleaned_length'] = processed_df['cleaned_text'].str.len()\n",
    "        processed_df['preprocessing_timestamp'] = pd.Timestamp.now()\n",
    "        \n",
    "        # In th·ªëng k√™\n",
    "        print(f\"   ‚úÖ X·ª≠ l√Ω ho√†n th√†nh:\")\n",
    "        print(f\"      - URLs lo·∫°i b·ªè: {self.cleaning_stats['urls_removed']}\")\n",
    "        print(f\"      - Mentions lo·∫°i b·ªè: {self.cleaning_stats['mentions_removed']}\")\n",
    "        print(f\"      - Hashtags lo·∫°i b·ªè: {self.cleaning_stats['hashtags_removed']}\")\n",
    "        print(f\"      - Texts qu√° ng·∫Øn: {self.cleaning_stats['texts_too_short']}\")\n",
    "        print(f\"      - Tweets lo·∫°i b·ªè: {removed_count}\")\n",
    "        print(f\"      - Tweets c√≤n l·∫°i: {len(processed_df)}\")\n",
    "        \n",
    "        return processed_df\n",
    "\n",
    "# Kh·ªüi t·∫°o preprocessor v√† x·ª≠ l√Ω d·ªØ li·ªáu\n",
    "preprocessor = TextPreprocessor()\n",
    "processed_data = preprocessor.preprocess_dataframe(raw_data)\n",
    "\n",
    "# Hi·ªÉn th·ªã v√≠ d·ª• tr∆∞·ªõc v√† sau khi l√†m s·∫°ch\n",
    "print(\"\\\\nüîç V√ç D·ª§ TR∆Ø·ªöC V√Ä SAU KHI L√ÄM S·∫†CH:\")\n",
    "sample_indices = [0, 5, 10]\n",
    "for i in sample_indices:\n",
    "    if i < len(processed_data):\n",
    "        original = raw_data.iloc[i]['text']\n",
    "        cleaned = processed_data.iloc[i]['cleaned_text']\n",
    "        print(f\"\\\\n[{i+1}] G·ªëc: {original}\")\n",
    "        print(f\"    S·∫°ch: {cleaned}\")\n",
    "\n",
    "# Th·ªëng k√™ ƒë·ªô d√†i vƒÉn b·∫£n\n",
    "print(\"\\\\nüìè TH·ªêNG K√ä ƒê·ªò D√ÄI VƒÇN B·∫¢N:\")\n",
    "print(f\"   ƒê·ªô d√†i trung b√¨nh (g·ªëc): {processed_data['original_length'].mean():.1f} k√Ω t·ª±\")\n",
    "print(f\"   ƒê·ªô d√†i trung b√¨nh (s·∫°ch): {processed_data['cleaned_length'].mean():.1f} k√Ω t·ª±\")\n",
    "print(f\"   Gi·∫£m: {((processed_data['original_length'].mean() - processed_data['cleaned_length'].mean()) / processed_data['original_length'].mean() * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f846c6",
   "metadata": {},
   "source": [
    "# 4. T·∫£i v√† C·∫•u h√¨nh M√¥ h√¨nh RoBERTa\n",
    "\n",
    "S·ª≠ d·ª•ng m√¥ h√¨nh `cardiffnlp/twitter-roberta-base-sentiment-latest` t·ª´ Hugging Face, gi·ªëng nh∆∞ ƒë·ªì √°n g·ªëc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f073d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C·∫•u h√¨nh m√¥ h√¨nh RoBERTa\n",
    "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "BATCH_SIZE = 16  # Gi·∫£m ƒë·ªÉ tr√°nh memory issues\n",
    "\n",
    "# Label mapping cho RoBERTa (theo ƒë·ªì √°n g·ªëc)\n",
    "SENTIMENT_LABEL_MAPPING = {\n",
    "    'LABEL_0': 'Negative',\n",
    "    'LABEL_1': 'Neutral', \n",
    "    'LABEL_2': 'Positive'\n",
    "}\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    Sentiment Analysis s·ª≠ d·ª•ng RoBERTa-Twitter\n",
    "    T∆∞∆°ng t·ª± Pandas UDF approach trong ƒë·ªì √°n g·ªëc nh∆∞ng ƒë∆°n gi·∫£n h√≥a\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = MODEL_NAME, batch_size: int = BATCH_SIZE):\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.pipeline = None\n",
    "        self.device = None\n",
    "        \n",
    "        self._setup_model()\n",
    "    \n",
    "    def _setup_model(self):\n",
    "        \"\"\"Kh·ªüi t·∫°o sentiment analysis pipeline\"\"\"\n",
    "        # Ki·ªÉm tra GPU\n",
    "        self.device = 0 if torch.cuda.is_available() else -1\n",
    "        device_name = \"GPU\" if self.device == 0 else \"CPU\"\n",
    "        \n",
    "        print(f\"ü§ñ ƒêang t·∫£i m√¥ h√¨nh RoBERTa: {self.model_name}\")\n",
    "        print(f\"   Device: {device_name}\")\n",
    "        \n",
    "        try:\n",
    "            # T·∫£i pipeline (gi·ªëng ƒë·ªì √°n g·ªëc)\n",
    "            self.pipeline = pipeline(\n",
    "                \"sentiment-analysis\",\n",
    "                model=self.model_name,\n",
    "                tokenizer=self.model_name,\n",
    "                device=self.device,\n",
    "                return_all_scores=True  # L·∫•y scores cho t·∫•t c·∫£ labels\n",
    "            )\n",
    "            \n",
    "            print(\"   ‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå L·ªói t·∫£i m√¥ h√¨nh: {e}\")\n",
    "            print(\"   üîÑ ƒêang th·ª≠ l·∫°i v·ªõi CPU...\")\n",
    "            \n",
    "            # Fallback to CPU\n",
    "            self.device = -1\n",
    "            self.pipeline = pipeline(\n",
    "                \"sentiment-analysis\",\n",
    "                model=self.model_name,\n",
    "                tokenizer=self.model_name,\n",
    "                device=self.device,\n",
    "                return_all_scores=True\n",
    "            )\n",
    "            print(\"   ‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c t·∫£i tr√™n CPU!\")\n",
    "    \n",
    "    def _process_predictions(self, predictions: List[List[Dict]]) -> List[Dict]:\n",
    "        \"\"\"X·ª≠ l√Ω k·∫øt qu·∫£ d·ª± ƒëo√°n th√†nh format d·ªÖ ƒë·ªçc\"\"\"\n",
    "        processed_results = []\n",
    "        \n",
    "        for pred_list in predictions:\n",
    "            # T√¨m prediction c√≥ score cao nh·∫•t\n",
    "            best_pred = max(pred_list, key=lambda x: x['score'])\n",
    "            \n",
    "            # Map label sang ƒë·ªãnh d·∫°ng d·ªÖ ƒë·ªçc\n",
    "            raw_label = best_pred['label']\n",
    "            readable_label = SENTIMENT_LABEL_MAPPING.get(raw_label, raw_label)\n",
    "            \n",
    "            # T·∫°o result dictionary\n",
    "            result = {\n",
    "                'sentiment_label': readable_label,\n",
    "                'sentiment_score': best_pred['score'],\n",
    "                'raw_label': raw_label,\n",
    "                'all_scores': {\n",
    "                    SENTIMENT_LABEL_MAPPING.get(item['label'], item['label']): item['score'] \n",
    "                    for item in pred_list\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            processed_results.append(result)\n",
    "        \n",
    "        return processed_results\n",
    "    \n",
    "    def predict_sentiment(self, texts: List[str]) -> List[Dict]:\n",
    "        \"\"\"D·ª± ƒëo√°n sentiment cho list texts v·ªõi batch processing\"\"\"\n",
    "        if not texts:\n",
    "            return []\n",
    "        \n",
    "        # L·ªçc texts h·ª£p l·ªá\n",
    "        valid_texts = [text for text in texts if text and text.strip()]\n",
    "        \n",
    "        if not valid_texts:\n",
    "            print(\"‚ö†Ô∏è Kh√¥ng c√≥ texts h·ª£p l·ªá ƒë·ªÉ ph√¢n t√≠ch\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"üîÑ ƒêang ph√¢n t√≠ch sentiment cho {len(valid_texts)} texts...\")\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # X·ª≠ l√Ω theo batch (gi·ªëng ƒë·ªì √°n g·ªëc)\n",
    "        from tqdm import tqdm\n",
    "        \n",
    "        for i in tqdm(range(0, len(valid_texts), self.batch_size), desc=\"Processing batches\"):\n",
    "            batch_texts = valid_texts[i:i + self.batch_size]\n",
    "            \n",
    "            try:\n",
    "                # G·ªçi model\n",
    "                batch_predictions = self.pipeline(batch_texts)\n",
    "                \n",
    "                # X·ª≠ l√Ω k·∫øt qu·∫£\n",
    "                batch_results = self._process_predictions(batch_predictions)\n",
    "                results.extend(batch_results)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå L·ªói x·ª≠ l√Ω batch {i//self.batch_size + 1}: {e}\")\n",
    "                # Th√™m k·∫øt qu·∫£ tr·ªëng cho batch b·ªã l·ªói\n",
    "                empty_results = [{\n",
    "                    'sentiment_label': 'Unknown',\n",
    "                    'sentiment_score': 0.0,\n",
    "                    'raw_label': 'ERROR',\n",
    "                    'all_scores': {}\n",
    "                }] * len(batch_texts)\n",
    "                results.extend(empty_results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_dataframe(self, df: pd.DataFrame, text_column: str = 'cleaned_text') -> pd.DataFrame:\n",
    "        \"\"\"Ph√¢n t√≠ch sentiment cho DataFrame (gi·ªëng Spark DataFrame processing)\"\"\"\n",
    "        if df.empty:\n",
    "            print(\"‚ö†Ô∏è DataFrame r·ªóng\")\n",
    "            return df\n",
    "        \n",
    "        if text_column not in df.columns:\n",
    "            print(f\"‚ùå Kh√¥ng t√¨m th·∫•y column '{text_column}'\")\n",
    "            return df\n",
    "        \n",
    "        print(f\"üöÄ B·∫Øt ƒë·∫ßu ph√¢n t√≠ch sentiment cho {len(df)} tweets...\")\n",
    "        \n",
    "        # T·∫°o copy\n",
    "        result_df = df.copy()\n",
    "        \n",
    "        # L·∫•y texts ƒë·ªÉ ph√¢n t√≠ch\n",
    "        texts = df[text_column].fillna('').astype(str).tolist()\n",
    "        \n",
    "        # Th·ª±c hi·ªán ph√¢n t√≠ch\n",
    "        predictions = self.predict_sentiment(texts)\n",
    "        \n",
    "        if predictions:\n",
    "            # Th√™m k·∫øt qu·∫£ v√†o DataFrame\n",
    "            result_df['sentiment_label'] = [pred['sentiment_label'] for pred in predictions]\n",
    "            result_df['sentiment_score'] = [pred['sentiment_score'] for pred in predictions]\n",
    "            result_df['raw_sentiment_label'] = [pred['raw_label'] for pred in predictions]\n",
    "            \n",
    "            # Th√™m scores ri√™ng l·∫ª\n",
    "            result_df['positive_score'] = [pred['all_scores'].get('Positive', 0.0) for pred in predictions]\n",
    "            result_df['negative_score'] = [pred['all_scores'].get('Negative', 0.0) for pred in predictions]\n",
    "            result_df['neutral_score'] = [pred['all_scores'].get('Neutral', 0.0) for pred in predictions]\n",
    "            \n",
    "            # Metadata\n",
    "            result_df['analysis_timestamp'] = pd.Timestamp.now()\n",
    "            result_df['model_used'] = self.model_name\n",
    "            \n",
    "            print(\"‚úÖ Ph√¢n t√≠ch ho√†n th√†nh!\")\n",
    "            \n",
    "            # Th·ªëng k√™ k·∫øt qu·∫£\n",
    "            sentiment_counts = result_df['sentiment_label'].value_counts()\n",
    "            print(\"\\\\nüìä Ph√¢n b·ªë sentiment:\")\n",
    "            for sentiment, count in sentiment_counts.items():\n",
    "                percentage = (count / len(result_df)) * 100\n",
    "                print(f\"   {sentiment}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        else:\n",
    "            print(\"‚ùå Kh√¥ng c√≥ k·∫øt qu·∫£ d·ª± ƒëo√°n\")\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "# Kh·ªüi t·∫°o analyzer\n",
    "print(\"ü§ñ Kh·ªüi t·∫°o Sentiment Analyzer...\")\n",
    "analyzer = SentimentAnalyzer()\n",
    "\n",
    "# Test v·ªõi m·ªôt v√†i c√¢u m·∫´u tr∆∞·ªõc\n",
    "test_texts = [\n",
    "    \"i love using gpt its amazing for coding\",\n",
    "    \"chatgpt is helpful but sometimes wrong\",\n",
    "    \"github copilot saves me time programming\"\n",
    "]\n",
    "\n",
    "print(\"\\\\nüß™ Test v·ªõi c√¢u m·∫´u:\")\n",
    "test_results = analyzer.predict_sentiment(test_texts)\n",
    "for text, result in zip(test_texts, test_results):\n",
    "    print(f\"   Text: {text}\")\n",
    "    print(f\"   ‚Üí {result['sentiment_label']} ({result['sentiment_score']:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049b886a",
   "metadata": {},
   "source": [
    "# 5. Th·ª±c hi·ªán Ph√¢n t√≠ch C·∫£m x√∫c\n",
    "\n",
    "√Åp d·ª•ng m√¥ h√¨nh RoBERTa ƒë·ªÉ d·ª± ƒëo√°n sentiment cho to√†n b·ªô dataset ƒë√£ ƒë∆∞·ª£c ti·ªÅn x·ª≠ l√Ω."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9a1ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Th·ª±c hi·ªán ph√¢n t√≠ch sentiment cho to√†n b·ªô dataset\n",
    "print(\"üöÄ B·∫ÆT ƒê·∫¶U PH√ÇN T√çCH SENTIMENT CHO TO√ÄN B·ªò DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "analyzed_data = analyzer.analyze_dataframe(processed_data, text_column='cleaned_text')\n",
    "\n",
    "print(\"\\\\n‚úÖ HO√ÄN TH√ÄNH PH√ÇN T√çCH SENTIMENT!\")\n",
    "print(f\"üìä T·ªïng s·ªë tweets ƒë√£ ph√¢n t√≠ch: {len(analyzed_data)}\")\n",
    "\n",
    "# Hi·ªÉn th·ªã th·ªëng k√™ chi ti·∫øt\n",
    "print(\"\\\\nüìà TH·ªêNG K√ä CHI TI·∫æT:\")\n",
    "\n",
    "# 1. Ph√¢n b·ªë sentiment t·ªïng th·ªÉ\n",
    "overall_sentiment = analyzed_data['sentiment_label'].value_counts()\n",
    "print(\"\\\\n1Ô∏è‚É£ Ph√¢n b·ªë sentiment t·ªïng th·ªÉ:\")\n",
    "for sentiment, count in overall_sentiment.items():\n",
    "    percentage = (count / len(analyzed_data)) * 100\n",
    "    print(f\"   {sentiment}: {count} tweets ({percentage:.1f}%)\")\n",
    "\n",
    "# 2. Ph√¢n b·ªë sentiment theo t·ª´ kh√≥a\n",
    "print(\"\\\\n2Ô∏è‚É£ Ph√¢n b·ªë sentiment theo t·ª´ kh√≥a:\")\n",
    "sentiment_by_keyword = analyzed_data.groupby(['keyword', 'sentiment_label']).size().unstack(fill_value=0)\n",
    "print(sentiment_by_keyword)\n",
    "\n",
    "# 3. ƒêi·ªÉm confidence trung b√¨nh\n",
    "print(\"\\\\n3Ô∏è‚É£ ƒêi·ªÉm confidence trung b√¨nh:\")\n",
    "avg_confidence = analyzed_data.groupby('sentiment_label')['sentiment_score'].mean()\n",
    "for sentiment, score in avg_confidence.items():\n",
    "    print(f\"   {sentiment}: {score:.3f}\")\n",
    "\n",
    "# 4. Top tweets cho m·ªói sentiment\n",
    "print(\"\\\\n4Ô∏è‚É£ V√≠ d·ª• tweets cho m·ªói sentiment:\")\n",
    "for sentiment in ['Positive', 'Negative', 'Neutral']:\n",
    "    if sentiment in analyzed_data['sentiment_label'].values:\n",
    "        sample = analyzed_data[analyzed_data['sentiment_label'] == sentiment].iloc[0]\n",
    "        print(f\"\\\\n   {sentiment.upper()}:\")\n",
    "        print(f\"   Original: {sample['text'][:100]}...\")\n",
    "        print(f\"   Cleaned:  {sample['cleaned_text'][:100]}...\")\n",
    "        print(f\"   Score:    {sample['sentiment_score']:.3f}\")\n",
    "        print(f\"   Keyword:  {sample['keyword']}\")\n",
    "\n",
    "# 5. L∆∞u k·∫øt qu·∫£\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = f\"tweets_analyzed_{timestamp}.csv\"\n",
    "\n",
    "# Ch·ªçn columns quan tr·ªçng ƒë·ªÉ l∆∞u\n",
    "columns_to_save = [\n",
    "    'id', 'text', 'cleaned_text', 'keyword', 'created_at', 'username',\n",
    "    'sentiment_label', 'sentiment_score', 'positive_score', 'negative_score', 'neutral_score',\n",
    "    'retweet_count', 'like_count'\n",
    "]\n",
    "\n",
    "analyzed_data[columns_to_save].to_csv(output_file, index=False, encoding='utf-8')\n",
    "print(f\"\\\\nüíæ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o file: {output_file}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 60)\n",
    "print(\"üéâ PH√ÇN T√çCH SENTIMENT HO√ÄN T·∫§T!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b39d2",
   "metadata": {},
   "source": [
    "# 6. Ph√¢n t√≠ch K·∫øt qu·∫£ v√† Tr·ª±c quan h√≥a\n",
    "\n",
    "T·∫°o c√°c bi·ªÉu ƒë·ªì t∆∞∆°ng t·ª± nh∆∞ ƒë·ªì √°n g·ªëc (Figures 6.4, 6.5) ƒë·ªÉ so s√°nh v√† ph√¢n t√≠ch k·∫øt qu·∫£."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba9cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C·∫•u h√¨nh cho visualization\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Color mapping cho sentiments\n",
    "sentiment_colors = {\n",
    "    'Positive': '#2ca02c',    # Green\n",
    "    'Negative': '#d62728',    # Red  \n",
    "    'Neutral': '#ff7f0e'      # Orange\n",
    "}\n",
    "\n",
    "print(\"üé® TR·ª∞C QUAN H√ìA K·∫æT QU·∫¢ - T∆Ø∆†NG T·ª∞ ƒê·ªí √ÅN G·ªêC\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Bi·ªÉu ƒë·ªì c·ªôt ch·ªìng - Sentiment Distribution by Keyword (gi·ªëng Figure 6.4)\n",
    "print(\"\\\\n1Ô∏è‚É£ T·∫°o bi·ªÉu ƒë·ªì ph√¢n b·ªë sentiment theo t·ª´ kh√≥a...\")\n",
    "\n",
    "# T√≠nh t·ª∑ l·ªá ph·∫ßn trƒÉm\n",
    "sentiment_by_keyword = analyzed_data.groupby(['keyword', 'sentiment_label']).size().unstack(fill_value=0)\n",
    "sentiment_percentages = sentiment_by_keyword.div(sentiment_by_keyword.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# T·∫°o bi·ªÉu ƒë·ªì\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "colors = [sentiment_colors.get(col, '#808080') for col in sentiment_percentages.columns]\n",
    "sentiment_percentages.plot(kind='bar', stacked=True, ax=ax, color=colors, width=0.7)\n",
    "\n",
    "ax.set_title('Sentiment Distribution by Keyword\\\\n(T∆∞∆°ng t·ª± Figure 6.4 - ƒê·ªì √°n g·ªëc)', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Keywords', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Percentage (%)', fontsize=12, fontweight='bold')\n",
    "ax.legend(title='Sentiment', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax.set_xticklabels(sentiment_percentages.index, rotation=45, ha='right')\n",
    "\n",
    "# Th√™m labels tr√™n bars\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.1f%%', label_type='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Bi·ªÉu ƒë·ªì t·ªïng quan sentiment (gi·ªëng Figure 6.5)\n",
    "print(\"\\\\n2Ô∏è‚É£ T·∫°o bi·ªÉu ƒë·ªì t·ªïng quan sentiment...\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Pie chart\n",
    "sentiment_counts = analyzed_data['sentiment_label'].value_counts()\n",
    "colors = [sentiment_colors.get(sentiment, '#808080') for sentiment in sentiment_counts.index]\n",
    "wedges, texts, autotexts = ax1.pie(sentiment_counts.values, labels=sentiment_counts.index, \n",
    "                                  autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "ax1.set_title('Overall Sentiment Distribution\\\\n(T∆∞∆°ng t·ª± Figure 6.5)', fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "bars = ax2.bar(sentiment_counts.index, sentiment_counts.values, \n",
    "               color=[sentiment_colors.get(sentiment, '#808080') for sentiment in sentiment_counts.index],\n",
    "               alpha=0.8)\n",
    "ax2.set_title('Sentiment Counts', fontweight='bold')\n",
    "ax2.set_xlabel('Sentiment')\n",
    "ax2.set_ylabel('Number of Tweets')\n",
    "\n",
    "# Th√™m labels tr√™n bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01*max(sentiment_counts),\n",
    "             f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Heatmap correlation gi·ªØa keywords v√† sentiments\n",
    "print(\"\\\\n3Ô∏è‚É£ T·∫°o heatmap correlation...\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Chuy·ªÉn v·ªÅ counts ƒë·ªÉ d·ªÖ ƒë·ªçc\n",
    "heatmap_data = analyzed_data.groupby(['keyword', 'sentiment_label']).size().unstack(fill_value=0)\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='d', cmap='RdYlGn', center=heatmap_data.mean().mean())\n",
    "plt.title('Sentiment Counts by Keyword (Heatmap)', fontweight='bold', pad=20)\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Keywords')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Box plot cho sentiment scores\n",
    "print(\"\\\\n4Ô∏è‚É£ T·∫°o box plot cho sentiment scores...\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "analyzed_data.boxplot(column='sentiment_score', by='sentiment_label', \n",
    "                     figsize=(12, 6), patch_artist=True)\n",
    "plt.title('Distribution of Confidence Scores by Sentiment')\n",
    "plt.xlabel('Sentiment Label')\n",
    "plt.ylabel('Confidence Score')\n",
    "plt.suptitle('')  # Remove automatic title\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\\\n‚úÖ Ho√†n th√†nh t·∫•t c·∫£ bi·ªÉu ƒë·ªì!\")\n",
    "\n",
    "# 5. T·∫°o b·∫£ng so s√°nh v·ªõi ƒë·ªì √°n g·ªëc (n·∫øu c√≥ d·ªØ li·ªáu)\n",
    "print(\"\\\\n5Ô∏è‚É£ So s√°nh v·ªõi k·∫øt qu·∫£ ƒë·ªì √°n g·ªëc:\")\n",
    "print(\"\\\\nüìä K·∫æT QU·∫¢ HI·ªÜN T·∫†I:\")\n",
    "current_results = analyzed_data['sentiment_label'].value_counts(normalize=True) * 100\n",
    "for sentiment, percentage in current_results.items():\n",
    "    print(f\"   {sentiment}: {percentage:.1f}%\")\n",
    "\n",
    "print(\"\\\\nüìö SO S√ÅNH V·ªöI ƒê·ªí √ÅN G·ªêC:\")\n",
    "print(\"   - ƒê·ªì √°n g·ªëc ƒë·∫°t ~82% accuracy tr√™n RoBERTa\")\n",
    "print(\"   - S·ª≠ d·ª•ng c√πng model: cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "print(\"   - Pipeline t∆∞∆°ng t·ª±: Text cleaning ‚Üí RoBERTa ‚Üí Classification\")\n",
    "print(\"   - Kh√°c bi·ªát: ƒê∆°n gi·∫£n h√≥a ki·∫øn tr√∫c (Python vs Spark)\")\n",
    "\n",
    "# 6. Th·ªëng k√™ n√¢ng cao\n",
    "print(\"\\\\n6Ô∏è‚É£ Th·ªëng k√™ n√¢ng cao:\")\n",
    "print(f\"   Average confidence score: {analyzed_data['sentiment_score'].mean():.3f}\")\n",
    "print(f\"   Tweets v·ªõi high confidence (>0.8): {(analyzed_data['sentiment_score'] > 0.8).sum()} ({(analyzed_data['sentiment_score'] > 0.8).mean()*100:.1f}%)\")\n",
    "print(f\"   T·ª´ kh√≥a c√≥ sentiment t√≠ch c·ª±c nh·∫•t: {sentiment_by_keyword.loc[:, 'Positive'].idxmax()}\")\n",
    "print(f\"   T·ª´ kh√≥a c√≥ sentiment ti√™u c·ª±c nh·∫•t: {sentiment_by_keyword.loc[:, 'Negative'].idxmax()}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 60)\n",
    "print(\"üéØ TR·ª∞C QUAN H√ìA HO√ÄN T·∫§T!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fbd1dc",
   "metadata": {},
   "source": [
    "# 7. ƒê√°nh gi√° Hi·ªáu su·∫•t v√† So s√°nh\n",
    "\n",
    "Ph√¢n t√≠ch hi·ªáu su·∫•t c·ªßa m√¥ h√¨nh v√† so s√°nh v·ªõi k·∫øt qu·∫£ t·ª´ ƒë·ªì √°n g·ªëc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84369012",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìà ƒê√ÅNH GI√Å HI·ªÜU SU·∫§T V√Ä SO S√ÅNH V·ªöI ƒê·ªí √ÅN G·ªêC\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Ph√¢n t√≠ch performance metrics\n",
    "print(\"\\\\n1Ô∏è‚É£ METRICS HI·ªÜU SU·∫§T:\")\n",
    "\n",
    "# Confidence score distribution\n",
    "confidence_stats = analyzed_data['sentiment_score'].describe()\n",
    "print(\"\\\\nüìä Ph√¢n b·ªë confidence scores:\")\n",
    "for stat, value in confidence_stats.items():\n",
    "    print(f\"   {stat}: {value:.4f}\")\n",
    "\n",
    "# High confidence predictions\n",
    "high_conf_threshold = 0.8\n",
    "high_conf_count = (analyzed_data['sentiment_score'] > high_conf_threshold).sum()\n",
    "high_conf_percentage = (high_conf_count / len(analyzed_data)) * 100\n",
    "\n",
    "print(f\"\\\\nüéØ Predictions v·ªõi high confidence (>{high_conf_threshold}):\")\n",
    "print(f\"   Count: {high_conf_count}/{len(analyzed_data)} ({high_conf_percentage:.1f}%)\")\n",
    "\n",
    "# Confidence by sentiment\n",
    "print(\"\\\\nüìà Average confidence by sentiment:\")\n",
    "conf_by_sentiment = analyzed_data.groupby('sentiment_label')['sentiment_score'].agg(['mean', 'std', 'count'])\n",
    "print(conf_by_sentiment.round(4))\n",
    "\n",
    "# 2. So s√°nh v·ªõi ƒë·ªì √°n g·ªëc\n",
    "print(\"\\\\n\\\\n2Ô∏è‚É£ SO S√ÅNH V·ªöI ƒê·ªí √ÅN G·ªêC:\")\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "print(\"| ASPECT | ƒê·ªí √ÅN G·ªêC | D·ª∞ √ÅN HI·ªÜN T·∫†I |\")\n",
    "print(\"=\"*50)\n",
    "print(\"| Ki·∫øn tr√∫c | Kafka+Spark+MongoDB | Python Pipeline |\")\n",
    "print(\"| M√¥ h√¨nh | RoBERTa-Twitter | RoBERTa-Twitter |\")\n",
    "print(\"| Accuracy | ~82% | Kh√¥ng c√≥ ground truth |\")\n",
    "print(\"| X·ª≠ l√Ω | Distributed | Single machine |\")\n",
    "print(\"| Real-time | Yes | Batch processing |\")\n",
    "print(\"| Complexity | High | Low |\")\n",
    "print(\"| Scalability | High | Medium |\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 3. Insights t·ª´ k·∫øt qu·∫£\n",
    "print(\"\\\\n\\\\n3Ô∏è‚É£ INSIGHTS V√Ä PH√ÇN T√çCH:\")\n",
    "\n",
    "# Keyword analysis\n",
    "keyword_sentiment = analyzed_data.groupby('keyword')['sentiment_label'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "print(\"\\\\nüìä Sentiment ratios by keyword:\")\n",
    "print(keyword_sentiment.round(3))\n",
    "\n",
    "# Most positive/negative keywords\n",
    "if 'Positive' in keyword_sentiment.columns:\n",
    "    most_positive = keyword_sentiment['Positive'].idxmax()\n",
    "    print(f\"\\\\nüòä Most positive keyword: {most_positive} ({keyword_sentiment['Positive'][most_positive]:.1%} positive)\")\n",
    "\n",
    "if 'Negative' in keyword_sentiment.columns:\n",
    "    most_negative = keyword_sentiment['Negative'].idxmax()\n",
    "    print(f\"üòû Most negative keyword: {most_negative} ({keyword_sentiment['Negative'][most_negative]:.1%} negative)\")\n",
    "\n",
    "# 4. T·∫°o summary report\n",
    "print(\"\\\\n\\\\n4Ô∏è‚É£ SUMMARY REPORT:\")\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"üéØ TWITTER SENTIMENT ANALYSIS - FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Dataset info\n",
    "print(f\"\\\\nüìä DATASET INFO:\")\n",
    "print(f\"   Total tweets analyzed: {len(analyzed_data)}\")\n",
    "print(f\"   Keywords: {', '.join(KEYWORDS)}\")\n",
    "print(f\"   Date range: {analyzed_data['created_at'].min().date()} to {analyzed_data['created_at'].max().date()}\")\n",
    "\n",
    "# Model info\n",
    "print(f\"\\\\nü§ñ MODEL INFO:\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Device: {'GPU' if analyzer.device == 0 else 'CPU'}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Results summary\n",
    "print(f\"\\\\nüìà RESULTS SUMMARY:\")\n",
    "overall_sentiment = analyzed_data['sentiment_label'].value_counts(normalize=True) * 100\n",
    "for sentiment, percentage in overall_sentiment.items():\n",
    "    print(f\"   {sentiment}: {percentage:.1f}%\")\n",
    "\n",
    "print(f\"\\\\n‚≠ê QUALITY METRICS:\")\n",
    "print(f\"   Average confidence: {analyzed_data['sentiment_score'].mean():.3f}\")\n",
    "print(f\"   High confidence predictions: {high_conf_percentage:.1f}%\")\n",
    "print(f\"   Processing time: ~{len(analyzed_data) / 60:.1f} tweets/minute\")\n",
    "\n",
    "# 5. Recommendations for improvement\n",
    "print(\"\\\\n\\\\n5Ô∏è‚É£ KHUY·∫æN NGH·ªä C·∫¢I TI·∫æN:\")\n",
    "print(\"\\\\nüîß Technical improvements:\")\n",
    "print(\"   ‚Ä¢ Th√™m data validation v√† error handling\")\n",
    "print(\"   ‚Ä¢ Implement caching cho model predictions\")\n",
    "print(\"   ‚Ä¢ S·ª≠ d·ª•ng GPU ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô x·ª≠ l√Ω\")\n",
    "print(\"   ‚Ä¢ Th√™m real-time streaming capabilities\")\n",
    "\n",
    "print(\"\\\\nüìä Analysis improvements:\")\n",
    "print(\"   ‚Ä¢ Thu th·∫≠p ground truth data ƒë·ªÉ ƒë√°nh gi√° accuracy\")\n",
    "print(\"   ‚Ä¢ Th√™m temporal analysis (xu h∆∞·ªõng theo th·ªùi gian)\")\n",
    "print(\"   ‚Ä¢ Ph√¢n t√≠ch deeper insights (hashtags, mentions)\")\n",
    "print(\"   ‚Ä¢ A/B testing v·ªõi c√°c models kh√°c\")\n",
    "\n",
    "print(\"\\\\nüöÄ Scaling improvements:\")\n",
    "print(\"   ‚Ä¢ Containerization v·ªõi Docker\")\n",
    "print(\"   ‚Ä¢ Deploy l√™n cloud (AWS/Azure)\")\n",
    "print(\"   ‚Ä¢ Implement proper logging v√† monitoring\")\n",
    "print(\"   ‚Ä¢ T·∫°o web dashboard v·ªõi Streamlit\")\n",
    "\n",
    "# 6. Export final results\n",
    "final_results = {\n",
    "    'total_tweets': len(analyzed_data),\n",
    "    'sentiment_distribution': analyzed_data['sentiment_label'].value_counts().to_dict(),\n",
    "    'sentiment_percentages': (analyzed_data['sentiment_label'].value_counts(normalize=True) * 100).round(2).to_dict(),\n",
    "    'average_confidence': float(analyzed_data['sentiment_score'].mean()),\n",
    "    'high_confidence_rate': float(high_conf_percentage),\n",
    "    'keywords_analyzed': KEYWORDS,\n",
    "    'model_used': MODEL_NAME,\n",
    "    'processing_timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "import json\n",
    "summary_file = f\"analysis_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\\\nüíæ Final summary saved to: {summary_file}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*70)\n",
    "print(\"üéâ PH√ÇN T√çCH HO√ÄN T·∫§T - TH√ÄNH C√îNG T√ÅI HI·ªÜN ƒê·ªí √ÅN!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Display success message\n",
    "print(\"\\\\nüèÜ TH√ÄNH C√îNG:\")\n",
    "print(\"   ‚úÖ Thu th·∫≠p d·ªØ li·ªáu t·ª´ Twitter (ho·∫∑c t·∫°o d·ªØ li·ªáu m·∫´u)\")\n",
    "print(\"   ‚úÖ Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n theo ƒë√∫ng pipeline ƒë·ªì √°n g·ªëc\")\n",
    "print(\"   ‚úÖ √Åp d·ª•ng m√¥ h√¨nh RoBERTa-Twitter t·ª´ Hugging Face\")\n",
    "print(\"   ‚úÖ T·∫°o tr·ª±c quan h√≥a t∆∞∆°ng t·ª± Figures 6.4, 6.5\")\n",
    "print(\"   ‚úÖ Ph√¢n t√≠ch v√† so s√°nh k·∫øt qu·∫£ v·ªõi ƒë·ªì √°n g·ªëc\")\n",
    "print(\"   ‚úÖ ƒê∆°n gi·∫£n h√≥a ki·∫øn tr√∫c nh∆∞ng v·∫´n ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng\")\n",
    "\n",
    "print(\"\\\\nüéØ D·ª± √°n ƒë√£ s·∫µn s√†ng ƒë·ªÉ tr√¨nh b√†y v√† b√°o c√°o!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
