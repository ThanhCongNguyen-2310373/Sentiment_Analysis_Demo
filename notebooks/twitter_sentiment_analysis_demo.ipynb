{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daf9fad9",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis vá»›i RoBERTa Model\n",
    "\n",
    "## TÃ¡i hiá»‡n Ä‘á»“ Ã¡n NLP phÃ¢n tÃ­ch cáº£m xÃºc vá»›i kiáº¿n trÃºc Ä‘Æ¡n giáº£n hÃ³a\n",
    "\n",
    "**Dá»± Ã¡n nÃ y tÃ¡i hiá»‡n vÃ  má»Ÿ rá»™ng nghiÃªn cá»©u vá» phÃ¢n tÃ­ch cáº£m xÃºc Twitter sá»­ dá»¥ng mÃ´ hÃ¬nh RoBERTa-Twitter, dá»±a trÃªn Ä‘á»“ Ã¡n gá»‘c cá»§a anh LÃ¢m Táº¥n Thá»‹nh.**\n",
    "\n",
    "### ğŸ¯ **Má»¥c tiÃªu**\n",
    "- TÃ¡i hiá»‡n mÃ´ hÃ¬nh AI phÃ¢n tÃ­ch cáº£m xÃºc vá»›i dá»¯ liá»‡u má»›i\n",
    "- Sá»­ dá»¥ng RoBERTa-Twitter model tá»« Hugging Face\n",
    "- ÄÆ¡n giáº£n hÃ³a kiáº¿n trÃºc: Python + Pandas + Transformers (thay vÃ¬ Kafka + Spark + MongoDB)\n",
    "- Táº¡o trá»±c quan hÃ³a tÆ°Æ¡ng tá»± Ä‘á»“ Ã¡n gá»‘c\n",
    "\n",
    "### ğŸ”„ **Quy trÃ¬nh**\n",
    "```\n",
    "Crawl Data â†’ Text Preprocessing â†’ RoBERTa Analysis â†’ Visualization â†’ Report\n",
    "```\n",
    "\n",
    "### ğŸ“Š **So sÃ¡nh kiáº¿n trÃºc**\n",
    "| Äá»“ Ã¡n gá»‘c | Dá»± Ã¡n hiá»‡n táº¡i |\n",
    "|-----------|----------------|\n",
    "| Producer â†’ Kafka â†’ Spark â†’ MongoDB | Python Script â†’ CSV â†’ RoBERTa â†’ Charts |\n",
    "| Big Data Architecture | Simplified Pipeline |\n",
    "| Distributed Processing | Single Machine |\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook nÃ y sáº½ hÆ°á»›ng dáº«n chi tiáº¿t tá»«ng bÆ°á»›c Ä‘á»ƒ xÃ¢y dá»±ng há»‡ thá»‘ng phÃ¢n tÃ­ch cáº£m xÃºc hoÃ n chá»‰nh.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89be4a5",
   "metadata": {},
   "source": [
    "# 1. Thiáº¿t láº­p MÃ´i trÆ°á»ng vÃ  CÃ i Ä‘áº·t ThÆ° viá»‡n\n",
    "\n",
    "Äáº§u tiÃªn, chÃºng ta sáº½ cÃ i Ä‘áº·t vÃ  import cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t cho dá»± Ã¡n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f54babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t (cháº¡y láº§n Ä‘áº§u)\n",
    "# !pip install pandas requests transformers torch matplotlib seaborn plotly tqdm python-dotenv\n",
    "\n",
    "# Import cÃ¡c thÆ° viá»‡n chÃ­nh\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Any\n",
    "\n",
    "# ThÆ° viá»‡n cho machine learning vÃ  NLP\n",
    "try:\n",
    "    from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "    import torch\n",
    "    print(\"âœ… Transformers vÃ  PyTorch Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "except ImportError as e:\n",
    "    print(\"âŒ Lá»—i import transformers/torch:\", e)\n",
    "    print(\"Vui lÃ²ng cháº¡y: !pip install transformers torch\")\n",
    "\n",
    "# ThÆ° viá»‡n cho visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Thiáº¿t láº­p style cho plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸš€ THIáº¾T Láº¬P HOÃ€N Táº¤T\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c979e2d9",
   "metadata": {},
   "source": [
    "# 2. Thu tháº­p Dá»¯ liá»‡u tá»« Twitter API\n",
    "\n",
    "ChÃºng ta sáº½ sá»­ dá»¥ng twitterapi.io Ä‘á»ƒ thu tháº­p tweets vá»›i cÃ¡c tá»« khÃ³a AI phá»• biáº¿n, tÆ°Æ¡ng tá»± nhÆ° Ä‘á»“ Ã¡n gá»‘c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beceab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cáº¥u hÃ¬nh API vÃ  tá»« khÃ³a (giá»‘ng Ä‘á»“ Ã¡n gá»‘c + thÃªm tá»« khÃ³a má»›i)\n",
    "TWITTER_API_KEY = \"your_api_key_here\"  # Thay tháº¿ báº±ng API key thá»±c tá»« twitterapi.io\n",
    "TWITTER_API_BASE_URL = \"https://api.twitterapi.io/v1\"\n",
    "\n",
    "# Tá»« khÃ³a tá»« Ä‘á»“ Ã¡n gá»‘c + tá»« khÃ³a má»›i\n",
    "KEYWORDS = [\n",
    "    \"GPT\", \"Copilot\", \"Gemini\",      # Tá»« Ä‘á»“ Ã¡n gá»‘c cá»§a anh Thá»‹nh\n",
    "    \"GPT-4o\", \"Sora\", \"Llama 3\",     # Tá»« khÃ³a AI má»›i\n",
    "    \"Claude\", \"ChatGPT\"              # Bá»• sung thÃªm\n",
    "]\n",
    "\n",
    "MAX_TWEETS_PER_KEYWORD = 100  # Giáº£m Ä‘á»ƒ demo nhanh\n",
    "RATE_LIMIT_DELAY = 2\n",
    "\n",
    "print(\"ğŸ“‹ Cáº¥u hÃ¬nh thu tháº­p dá»¯ liá»‡u:\")\n",
    "print(f\"   Keywords: {KEYWORDS}\")\n",
    "print(f\"   Max tweets per keyword: {MAX_TWEETS_PER_KEYWORD}\")\n",
    "print(f\"   Total expected tweets: {len(KEYWORDS) * MAX_TWEETS_PER_KEYWORD}\")\n",
    "\n",
    "class TwitterCrawler:\n",
    "    \"\"\"\n",
    "    Twitter data crawler sá»­ dá»¥ng twitterapi.io\n",
    "    TÆ°Æ¡ng tá»± Ä‘á»“ Ã¡n gá»‘c nhÆ°ng Ä‘Æ¡n giáº£n hÃ³a\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = TWITTER_API_BASE_URL\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'Authorization': f'Bearer {api_key}',\n",
    "            'Content-Type': 'application/json'\n",
    "        })\n",
    "    \n",
    "    def search_tweets(self, query: str, max_results: int = 100) -> List[Dict]:\n",
    "        \"\"\"Thu tháº­p tweets theo tá»« khÃ³a\"\"\"\n",
    "        endpoint = f\"{self.base_url}/search\"\n",
    "        \n",
    "        params = {\n",
    "            'query': query,\n",
    "            'max_results': min(max_results, 100),\n",
    "            'tweet.fields': 'created_at,author_id,public_metrics,lang',\n",
    "            'user.fields': 'name,username,verified',\n",
    "            'expansions': 'author_id'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(endpoint, params=params)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            tweets = []\n",
    "            \n",
    "            if 'data' in data:\n",
    "                # Táº¡o mapping user info\n",
    "                users = {}\n",
    "                if 'includes' in data and 'users' in data['includes']:\n",
    "                    users = {user['id']: user for user in data['includes']['users']}\n",
    "                \n",
    "                for tweet in data['data']:\n",
    "                    user_info = users.get(tweet.get('author_id', ''), {})\n",
    "                    \n",
    "                    tweet_data = {\n",
    "                        'id': tweet.get('id', ''),\n",
    "                        'text': tweet.get('text', ''),\n",
    "                        'created_at': tweet.get('created_at', ''),\n",
    "                        'username': user_info.get('username', ''),\n",
    "                        'user_name': user_info.get('name', ''),\n",
    "                        'retweet_count': tweet.get('public_metrics', {}).get('retweet_count', 0),\n",
    "                        'like_count': tweet.get('public_metrics', {}).get('like_count', 0),\n",
    "                        'lang': tweet.get('lang', ''),\n",
    "                        'keyword': query\n",
    "                    }\n",
    "                    tweets.append(tweet_data)\n",
    "            \n",
    "            return tweets\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"âŒ Lá»—i API cho tá»« khÃ³a '{query}': {e}\")\n",
    "            return []\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"âŒ Lá»—i parse JSON cho tá»« khÃ³a '{query}': {e}\")\n",
    "            return []\n",
    "    \n",
    "    def crawl_multiple_keywords(self, keywords: List[str], max_tweets: int) -> pd.DataFrame:\n",
    "        \"\"\"Thu tháº­p tweets cho nhiá»u tá»« khÃ³a\"\"\"\n",
    "        all_tweets = []\n",
    "        \n",
    "        print(f\"ğŸ”„ Báº¯t Ä‘áº§u thu tháº­p tweets cho {len(keywords)} tá»« khÃ³a...\")\n",
    "        \n",
    "        for i, keyword in enumerate(keywords, 1):\n",
    "            print(f\"   [{i}/{len(keywords)}] Äang crawl: '{keyword}'\")\n",
    "            \n",
    "            tweets = self.search_tweets(keyword, max_tweets)\n",
    "            \n",
    "            if tweets:\n",
    "                all_tweets.extend(tweets)\n",
    "                print(f\"      âœ… Thu tháº­p Ä‘Æ°á»£c {len(tweets)} tweets\")\n",
    "            else:\n",
    "                print(f\"      âš ï¸ KhÃ´ng thu tháº­p Ä‘Æ°á»£c tweets nÃ o\")\n",
    "            \n",
    "            # Rate limiting\n",
    "            if i < len(keywords):  # KhÃ´ng delay á»Ÿ láº§n cuá»‘i\n",
    "                time.sleep(RATE_LIMIT_DELAY)\n",
    "        \n",
    "        # Chuyá»ƒn thÃ nh DataFrame\n",
    "        df = pd.DataFrame(all_tweets)\n",
    "        \n",
    "        if not df.empty:\n",
    "            # Xá»­ lÃ½ dá»¯ liá»‡u\n",
    "            df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "            df = df.drop_duplicates(subset=['id'])  # Loáº¡i bá» duplicates\n",
    "            df = df[df['lang'] == 'en']  # Chá»‰ láº¥y tweets tiáº¿ng Anh\n",
    "            \n",
    "            print(f\"âœ… HoÃ n thÃ nh! Tá»•ng cá»™ng {len(df)} tweets unique (tiáº¿ng Anh)\")\n",
    "        else:\n",
    "            print(\"âŒ KhÃ´ng thu tháº­p Ä‘Æ°á»£c dá»¯ liá»‡u nÃ o\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Táº¡o crawler instance \n",
    "print(\"ğŸ¤– Khá»Ÿi táº¡o Twitter Crawler...\")\n",
    "if TWITTER_API_KEY == \"your_api_key_here\":\n",
    "    print(\"âš ï¸ Cáº¢NH BÃO: Báº¡n cáº§n thay tháº¿ TWITTER_API_KEY báº±ng API key thá»±c\")\n",
    "    print(\"   Äá»ƒ demo, chÃºng ta sáº½ táº¡o dá»¯ liá»‡u máº«u...\")\n",
    "    use_real_api = False\n",
    "else:\n",
    "    crawler = TwitterCrawler(TWITTER_API_KEY)\n",
    "    use_real_api = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f104fdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thu tháº­p dá»¯ liá»‡u thá»±c táº¿ hoáº·c táº¡o dá»¯ liá»‡u máº«u Ä‘á»ƒ demo\n",
    "if use_real_api:\n",
    "    # Sá»­ dá»¥ng API thá»±c\n",
    "    print(\"ğŸ”„ Thu tháº­p dá»¯ liá»‡u tá»« Twitter API...\")\n",
    "    raw_data = crawler.crawl_multiple_keywords(KEYWORDS, MAX_TWEETS_PER_KEYWORD)\n",
    "else:\n",
    "    # Táº¡o dá»¯ liá»‡u máº«u Ä‘á»ƒ demo\n",
    "    print(\"ğŸ”§ Táº¡o dá»¯ liá»‡u máº«u Ä‘á»ƒ demo...\")\n",
    "    \n",
    "    sample_tweets = [\n",
    "        \"I love using GPT-4! It's amazing for coding assistance and problem solving.\",\n",
    "        \"ChatGPT is helpful but sometimes gives wrong information. Need to be careful.\",\n",
    "        \"GitHub Copilot saves me so much time when programming. Highly recommend!\",\n",
    "        \"Gemini is decent but I still prefer GPT for most tasks.\",\n",
    "        \"The new GPT-4o model is incredibly fast and accurate. Impressed!\",\n",
    "        \"Sora AI video generation is mind-blowing. The future is here!\",\n",
    "        \"Llama 3 open source model is surprisingly good for a free alternative.\",\n",
    "        \"Claude is great for writing and analysis tasks. Very thoughtful responses.\",\n",
    "        \"AI copilots in coding are game changers. Can't imagine coding without them now.\",\n",
    "        \"These AI tools are making everyone more productive. Exciting times!\",\n",
    "        \"GPT sometimes hallucinates facts. Always double-check important information.\",\n",
    "        \"Copilot suggestions are usually good but sometimes completely off-topic.\",\n",
    "        \"I'm worried about AI replacing human creativity and jobs.\",\n",
    "        \"The quality of AI responses keeps getting better every month.\",\n",
    "        \"Using multiple AI tools together gives the best results for complex tasks.\"\n",
    "    ] * 10  # Repeat Ä‘á»ƒ cÃ³ Ä‘á»§ dá»¯ liá»‡u\n",
    "    \n",
    "    # Táº¡o DataFrame máº«u\n",
    "    import random\n",
    "    \n",
    "    raw_data = []\n",
    "    for i, text in enumerate(sample_tweets):\n",
    "        tweet_data = {\n",
    "            'id': f'tweet_{i}',\n",
    "            'text': text,\n",
    "            'created_at': pd.Timestamp.now() - pd.Timedelta(days=random.randint(0, 30)),\n",
    "            'username': f'user_{i % 20}',\n",
    "            'user_name': f'User {i % 20}',\n",
    "            'retweet_count': random.randint(0, 100),\n",
    "            'like_count': random.randint(0, 500),\n",
    "            'lang': 'en',\n",
    "            'keyword': random.choice(KEYWORDS)\n",
    "        }\n",
    "        raw_data.append(tweet_data)\n",
    "    \n",
    "    raw_data = pd.DataFrame(raw_data)\n",
    "    print(f\"âœ… Táº¡o thÃ nh cÃ´ng {len(raw_data)} tweets máº«u\")\n",
    "\n",
    "# Hiá»ƒn thá»‹ thÃ´ng tin dá»¯ liá»‡u thu tháº­p Ä‘Æ°á»£c\n",
    "print(\"\\\\nğŸ“Š THÃ”NG TIN Dá»® LIá»†U THU THáº¬P:\")\n",
    "print(f\"   Tá»•ng sá»‘ tweets: {len(raw_data)}\")\n",
    "print(f\"   Khoáº£ng thá»i gian: {raw_data['created_at'].min()} Ä‘áº¿n {raw_data['created_at'].max()}\")\n",
    "\n",
    "# PhÃ¢n bá»‘ theo tá»« khÃ³a\n",
    "keyword_distribution = raw_data['keyword'].value_counts()\n",
    "print(\"\\\\nğŸ“ˆ PhÃ¢n bá»‘ theo tá»« khÃ³a:\")\n",
    "for keyword, count in keyword_distribution.items():\n",
    "    print(f\"   {keyword}: {count} tweets\")\n",
    "\n",
    "# Hiá»ƒn thá»‹ máº«u dá»¯ liá»‡u\n",
    "print(\"\\\\nğŸ” Máº«u dá»¯ liá»‡u:\")\n",
    "print(raw_data[['text', 'username', 'keyword', 'created_at']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d0b42",
   "metadata": {},
   "source": [
    "# 3. Tiá»n xá»­ lÃ½ vÃ  LÃ m sáº¡ch VÄƒn báº£n\n",
    "\n",
    "Ãp dá»¥ng cÃ¡c bÆ°á»›c tiá»n xá»­ lÃ½ giá»‘ng nhÆ° Ä‘á»“ Ã¡n gá»‘c: chuyá»ƒn chá»¯ thÆ°á»ng, loáº¡i bá» URL, mentions, hashtags, kÃ½ tá»± Ä‘áº·c biá»‡t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c26b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Text preprocessing class theo Ä‘á»“ Ã¡n gá»‘c\n",
    "    Ãp dá»¥ng cÃ¡c bÆ°á»›c lÃ m sáº¡ch tÆ°Æ¡ng tá»± SQLTransformer trong Spark\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cleaning_stats = {\n",
    "            'urls_removed': 0,\n",
    "            'mentions_removed': 0,\n",
    "            'hashtags_removed': 0,\n",
    "            'texts_too_short': 0\n",
    "        }\n",
    "    \n",
    "    def remove_urls(self, text: str) -> str:\n",
    "        \"\"\"Loáº¡i bá» URLs\"\"\"\n",
    "        url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        if url_pattern.search(text):\n",
    "            self.cleaning_stats['urls_removed'] += 1\n",
    "        return url_pattern.sub('', text)\n",
    "    \n",
    "    def remove_mentions(self, text: str) -> str:\n",
    "        \"\"\"Loáº¡i bá» @mentions\"\"\"\n",
    "        mention_pattern = re.compile(r'@[\\\\w_]+')\n",
    "        if mention_pattern.search(text):\n",
    "            self.cleaning_stats['mentions_removed'] += 1\n",
    "        return mention_pattern.sub('', text)\n",
    "    \n",
    "    def remove_hashtags(self, text: str) -> str:\n",
    "        \"\"\"Loáº¡i bá» hashtags\"\"\"\n",
    "        hashtag_pattern = re.compile(r'#[\\\\w_]+')\n",
    "        if hashtag_pattern.search(text):\n",
    "            self.cleaning_stats['hashtags_removed'] += 1\n",
    "        return hashtag_pattern.sub('', text)\n",
    "    \n",
    "    def remove_special_chars(self, text: str) -> str:\n",
    "        \"\"\"Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t, chá»‰ giá»¯ chá»¯ cÃ¡i vÃ  sá»‘\"\"\"\n",
    "        return re.sub(r'[^a-zA-Z0-9\\\\s]', '', text)\n",
    "    \n",
    "    def remove_extra_whitespace(self, text: str) -> str:\n",
    "        \"\"\"Loáº¡i bá» khoáº£ng tráº¯ng thá»«a\"\"\"\n",
    "        return re.sub(r'\\\\s+', ' ', text).strip()\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Ãp dá»¥ng táº¥t cáº£ cÃ¡c bÆ°á»›c lÃ m sáº¡ch\n",
    "        Theo thá»© tá»± giá»‘ng Ä‘á»“ Ã¡n gá»‘c\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return \"\"\n",
    "        \n",
    "        # 1. Chuyá»ƒn chá»¯ thÆ°á»ng (giá»‘ng Ä‘á»“ Ã¡n gá»‘c)\n",
    "        text = text.lower()\n",
    "        \n",
    "        # 2. Loáº¡i bá» URLs\n",
    "        text = self.remove_urls(text)\n",
    "        \n",
    "        # 3. Loáº¡i bá» mentions vÃ  hashtags\n",
    "        text = self.remove_mentions(text)\n",
    "        text = self.remove_hashtags(text)\n",
    "        \n",
    "        # 4. Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t\n",
    "        text = self.remove_special_chars(text)\n",
    "        \n",
    "        # 5. Loáº¡i bá» khoáº£ng tráº¯ng thá»«a\n",
    "        text = self.remove_extra_whitespace(text)\n",
    "        \n",
    "        # 6. Kiá»ƒm tra Ä‘á»™ dÃ i tá»‘i thiá»ƒu\n",
    "        if len(text) < 10:\n",
    "            self.cleaning_stats['texts_too_short'] += 1\n",
    "            return \"\"\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def preprocess_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Xá»­ lÃ½ toÃ n bá»™ DataFrame\"\"\"\n",
    "        print(\"ğŸ§¹ Báº¯t Ä‘áº§u tiá»n xá»­ lÃ½ vÄƒn báº£n...\")\n",
    "        \n",
    "        # Reset stats\n",
    "        self.cleaning_stats = {key: 0 for key in self.cleaning_stats}\n",
    "        \n",
    "        # Táº¡o copy Ä‘á»ƒ khÃ´ng thay Ä‘á»•i dá»¯ liá»‡u gá»‘c\n",
    "        processed_df = df.copy()\n",
    "        \n",
    "        # Ãp dá»¥ng lÃ m sáº¡ch\n",
    "        processed_df['cleaned_text'] = processed_df['text'].apply(self.clean_text)\n",
    "        \n",
    "        # Loáº¡i bá» tweets rá»—ng sau khi lÃ m sáº¡ch\n",
    "        original_count = len(processed_df)\n",
    "        processed_df = processed_df[processed_df['cleaned_text'].str.len() > 0]\n",
    "        removed_count = original_count - len(processed_df)\n",
    "        \n",
    "        # ThÃªm metadata\n",
    "        processed_df['original_length'] = df['text'].str.len()\n",
    "        processed_df['cleaned_length'] = processed_df['cleaned_text'].str.len()\n",
    "        processed_df['preprocessing_timestamp'] = pd.Timestamp.now()\n",
    "        \n",
    "        # In thá»‘ng kÃª\n",
    "        print(f\"   âœ… Xá»­ lÃ½ hoÃ n thÃ nh:\")\n",
    "        print(f\"      - URLs loáº¡i bá»: {self.cleaning_stats['urls_removed']}\")\n",
    "        print(f\"      - Mentions loáº¡i bá»: {self.cleaning_stats['mentions_removed']}\")\n",
    "        print(f\"      - Hashtags loáº¡i bá»: {self.cleaning_stats['hashtags_removed']}\")\n",
    "        print(f\"      - Texts quÃ¡ ngáº¯n: {self.cleaning_stats['texts_too_short']}\")\n",
    "        print(f\"      - Tweets loáº¡i bá»: {removed_count}\")\n",
    "        print(f\"      - Tweets cÃ²n láº¡i: {len(processed_df)}\")\n",
    "        \n",
    "        return processed_df\n",
    "\n",
    "# Khá»Ÿi táº¡o preprocessor vÃ  xá»­ lÃ½ dá»¯ liá»‡u\n",
    "preprocessor = TextPreprocessor()\n",
    "processed_data = preprocessor.preprocess_dataframe(raw_data)\n",
    "\n",
    "# Hiá»ƒn thá»‹ vÃ­ dá»¥ trÆ°á»›c vÃ  sau khi lÃ m sáº¡ch\n",
    "print(\"\\\\nğŸ” VÃ Dá»¤ TRÆ¯á»šC VÃ€ SAU KHI LÃ€M Sáº CH:\")\n",
    "sample_indices = [0, 5, 10]\n",
    "for i in sample_indices:\n",
    "    if i < len(processed_data):\n",
    "        original = raw_data.iloc[i]['text']\n",
    "        cleaned = processed_data.iloc[i]['cleaned_text']\n",
    "        print(f\"\\\\n[{i+1}] Gá»‘c: {original}\")\n",
    "        print(f\"    Sáº¡ch: {cleaned}\")\n",
    "\n",
    "# Thá»‘ng kÃª Ä‘á»™ dÃ i vÄƒn báº£n\n",
    "print(\"\\\\nğŸ“ THá»NG KÃŠ Äá»˜ DÃ€I VÄ‚N Báº¢N:\")\n",
    "print(f\"   Äá»™ dÃ i trung bÃ¬nh (gá»‘c): {processed_data['original_length'].mean():.1f} kÃ½ tá»±\")\n",
    "print(f\"   Äá»™ dÃ i trung bÃ¬nh (sáº¡ch): {processed_data['cleaned_length'].mean():.1f} kÃ½ tá»±\")\n",
    "print(f\"   Giáº£m: {((processed_data['original_length'].mean() - processed_data['cleaned_length'].mean()) / processed_data['original_length'].mean() * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f846c6",
   "metadata": {},
   "source": [
    "# 4. Táº£i vÃ  Cáº¥u hÃ¬nh MÃ´ hÃ¬nh RoBERTa\n",
    "\n",
    "Sá»­ dá»¥ng mÃ´ hÃ¬nh `cardiffnlp/twitter-roberta-base-sentiment-latest` tá»« Hugging Face, giá»‘ng nhÆ° Ä‘á»“ Ã¡n gá»‘c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f073d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cáº¥u hÃ¬nh mÃ´ hÃ¬nh RoBERTa\n",
    "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "BATCH_SIZE = 16  # Giáº£m Ä‘á»ƒ trÃ¡nh memory issues\n",
    "\n",
    "# Label mapping cho RoBERTa (theo Ä‘á»“ Ã¡n gá»‘c)\n",
    "SENTIMENT_LABEL_MAPPING = {\n",
    "    'LABEL_0': 'Negative',\n",
    "    'LABEL_1': 'Neutral', \n",
    "    'LABEL_2': 'Positive'\n",
    "}\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    Sentiment Analysis sá»­ dá»¥ng RoBERTa-Twitter\n",
    "    TÆ°Æ¡ng tá»± Pandas UDF approach trong Ä‘á»“ Ã¡n gá»‘c nhÆ°ng Ä‘Æ¡n giáº£n hÃ³a\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = MODEL_NAME, batch_size: int = BATCH_SIZE):\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.pipeline = None\n",
    "        self.device = None\n",
    "        \n",
    "        self._setup_model()\n",
    "    \n",
    "    def _setup_model(self):\n",
    "        \"\"\"Khá»Ÿi táº¡o sentiment analysis pipeline\"\"\"\n",
    "        # Kiá»ƒm tra GPU\n",
    "        self.device = 0 if torch.cuda.is_available() else -1\n",
    "        device_name = \"GPU\" if self.device == 0 else \"CPU\"\n",
    "        \n",
    "        print(f\"ğŸ¤– Äang táº£i mÃ´ hÃ¬nh RoBERTa: {self.model_name}\")\n",
    "        print(f\"   Device: {device_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Táº£i pipeline (giá»‘ng Ä‘á»“ Ã¡n gá»‘c)\n",
    "            self.pipeline = pipeline(\n",
    "                \"sentiment-analysis\",\n",
    "                model=self.model_name,\n",
    "                tokenizer=self.model_name,\n",
    "                device=self.device,\n",
    "                return_all_scores=True  # Láº¥y scores cho táº¥t cáº£ labels\n",
    "            )\n",
    "            \n",
    "            print(\"   âœ… MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c táº£i thÃ nh cÃ´ng!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Lá»—i táº£i mÃ´ hÃ¬nh: {e}\")\n",
    "            print(\"   ğŸ”„ Äang thá»­ láº¡i vá»›i CPU...\")\n",
    "            \n",
    "            # Fallback to CPU\n",
    "            self.device = -1\n",
    "            self.pipeline = pipeline(\n",
    "                \"sentiment-analysis\",\n",
    "                model=self.model_name,\n",
    "                tokenizer=self.model_name,\n",
    "                device=self.device,\n",
    "                return_all_scores=True\n",
    "            )\n",
    "            print(\"   âœ… MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c táº£i trÃªn CPU!\")\n",
    "    \n",
    "    def _process_predictions(self, predictions: List[List[Dict]]) -> List[Dict]:\n",
    "        \"\"\"Xá»­ lÃ½ káº¿t quáº£ dá»± Ä‘oÃ¡n thÃ nh format dá»… Ä‘á»c\"\"\"\n",
    "        processed_results = []\n",
    "        \n",
    "        for pred_list in predictions:\n",
    "            # TÃ¬m prediction cÃ³ score cao nháº¥t\n",
    "            best_pred = max(pred_list, key=lambda x: x['score'])\n",
    "            \n",
    "            # Map label sang Ä‘á»‹nh dáº¡ng dá»… Ä‘á»c\n",
    "            raw_label = best_pred['label']\n",
    "            readable_label = SENTIMENT_LABEL_MAPPING.get(raw_label, raw_label)\n",
    "            \n",
    "            # Táº¡o result dictionary\n",
    "            result = {\n",
    "                'sentiment_label': readable_label,\n",
    "                'sentiment_score': best_pred['score'],\n",
    "                'raw_label': raw_label,\n",
    "                'all_scores': {\n",
    "                    SENTIMENT_LABEL_MAPPING.get(item['label'], item['label']): item['score'] \n",
    "                    for item in pred_list\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            processed_results.append(result)\n",
    "        \n",
    "        return processed_results\n",
    "    \n",
    "    def predict_sentiment(self, texts: List[str]) -> List[Dict]:\n",
    "        \"\"\"Dá»± Ä‘oÃ¡n sentiment cho list texts vá»›i batch processing\"\"\"\n",
    "        if not texts:\n",
    "            return []\n",
    "        \n",
    "        # Lá»c texts há»£p lá»‡\n",
    "        valid_texts = [text for text in texts if text and text.strip()]\n",
    "        \n",
    "        if not valid_texts:\n",
    "            print(\"âš ï¸ KhÃ´ng cÃ³ texts há»£p lá»‡ Ä‘á»ƒ phÃ¢n tÃ­ch\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"ğŸ”„ Äang phÃ¢n tÃ­ch sentiment cho {len(valid_texts)} texts...\")\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Xá»­ lÃ½ theo batch (giá»‘ng Ä‘á»“ Ã¡n gá»‘c)\n",
    "        from tqdm import tqdm\n",
    "        \n",
    "        for i in tqdm(range(0, len(valid_texts), self.batch_size), desc=\"Processing batches\"):\n",
    "            batch_texts = valid_texts[i:i + self.batch_size]\n",
    "            \n",
    "            try:\n",
    "                # Gá»i model\n",
    "                batch_predictions = self.pipeline(batch_texts)\n",
    "                \n",
    "                # Xá»­ lÃ½ káº¿t quáº£\n",
    "                batch_results = self._process_predictions(batch_predictions)\n",
    "                results.extend(batch_results)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Lá»—i xá»­ lÃ½ batch {i//self.batch_size + 1}: {e}\")\n",
    "                # ThÃªm káº¿t quáº£ trá»‘ng cho batch bá»‹ lá»—i\n",
    "                empty_results = [{\n",
    "                    'sentiment_label': 'Unknown',\n",
    "                    'sentiment_score': 0.0,\n",
    "                    'raw_label': 'ERROR',\n",
    "                    'all_scores': {}\n",
    "                }] * len(batch_texts)\n",
    "                results.extend(empty_results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_dataframe(self, df: pd.DataFrame, text_column: str = 'cleaned_text') -> pd.DataFrame:\n",
    "        \"\"\"PhÃ¢n tÃ­ch sentiment cho DataFrame (giá»‘ng Spark DataFrame processing)\"\"\"\n",
    "        if df.empty:\n",
    "            print(\"âš ï¸ DataFrame rá»—ng\")\n",
    "            return df\n",
    "        \n",
    "        if text_column not in df.columns:\n",
    "            print(f\"âŒ KhÃ´ng tÃ¬m tháº¥y column '{text_column}'\")\n",
    "            return df\n",
    "        \n",
    "        print(f\"ğŸš€ Báº¯t Ä‘áº§u phÃ¢n tÃ­ch sentiment cho {len(df)} tweets...\")\n",
    "        \n",
    "        # Táº¡o copy\n",
    "        result_df = df.copy()\n",
    "        \n",
    "        # Láº¥y texts Ä‘á»ƒ phÃ¢n tÃ­ch\n",
    "        texts = df[text_column].fillna('').astype(str).tolist()\n",
    "        \n",
    "        # Thá»±c hiá»‡n phÃ¢n tÃ­ch\n",
    "        predictions = self.predict_sentiment(texts)\n",
    "        \n",
    "        if predictions:\n",
    "            # ThÃªm káº¿t quáº£ vÃ o DataFrame\n",
    "            result_df['sentiment_label'] = [pred['sentiment_label'] for pred in predictions]\n",
    "            result_df['sentiment_score'] = [pred['sentiment_score'] for pred in predictions]\n",
    "            result_df['raw_sentiment_label'] = [pred['raw_label'] for pred in predictions]\n",
    "            \n",
    "            # ThÃªm scores riÃªng láº»\n",
    "            result_df['positive_score'] = [pred['all_scores'].get('Positive', 0.0) for pred in predictions]\n",
    "            result_df['negative_score'] = [pred['all_scores'].get('Negative', 0.0) for pred in predictions]\n",
    "            result_df['neutral_score'] = [pred['all_scores'].get('Neutral', 0.0) for pred in predictions]\n",
    "            \n",
    "            # Metadata\n",
    "            result_df['analysis_timestamp'] = pd.Timestamp.now()\n",
    "            result_df['model_used'] = self.model_name\n",
    "            \n",
    "            print(\"âœ… PhÃ¢n tÃ­ch hoÃ n thÃ nh!\")\n",
    "            \n",
    "            # Thá»‘ng kÃª káº¿t quáº£\n",
    "            sentiment_counts = result_df['sentiment_label'].value_counts()\n",
    "            print(\"\\\\nğŸ“Š PhÃ¢n bá»‘ sentiment:\")\n",
    "            for sentiment, count in sentiment_counts.items():\n",
    "                percentage = (count / len(result_df)) * 100\n",
    "                print(f\"   {sentiment}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        else:\n",
    "            print(\"âŒ KhÃ´ng cÃ³ káº¿t quáº£ dá»± Ä‘oÃ¡n\")\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "# Khá»Ÿi táº¡o analyzer\n",
    "print(\"ğŸ¤– Khá»Ÿi táº¡o Sentiment Analyzer...\")\n",
    "analyzer = SentimentAnalyzer()\n",
    "\n",
    "# Test vá»›i má»™t vÃ i cÃ¢u máº«u trÆ°á»›c\n",
    "test_texts = [\n",
    "    \"i love using gpt its amazing for coding\",\n",
    "    \"chatgpt is helpful but sometimes wrong\",\n",
    "    \"github copilot saves me time programming\"\n",
    "]\n",
    "\n",
    "print(\"\\\\nğŸ§ª Test vá»›i cÃ¢u máº«u:\")\n",
    "test_results = analyzer.predict_sentiment(test_texts)\n",
    "for text, result in zip(test_texts, test_results):\n",
    "    print(f\"   Text: {text}\")\n",
    "    print(f\"   â†’ {result['sentiment_label']} ({result['sentiment_score']:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049b886a",
   "metadata": {},
   "source": [
    "# 5. Thá»±c hiá»‡n PhÃ¢n tÃ­ch Cáº£m xÃºc\n",
    "\n",
    "Ãp dá»¥ng mÃ´ hÃ¬nh RoBERTa Ä‘á»ƒ dá»± Ä‘oÃ¡n sentiment cho toÃ n bá»™ dataset Ä‘Ã£ Ä‘Æ°á»£c tiá»n xá»­ lÃ½."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9a1ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thá»±c hiá»‡n phÃ¢n tÃ­ch sentiment cho toÃ n bá»™ dataset\n",
    "print(\"ğŸš€ Báº®T Äáº¦U PHÃ‚N TÃCH SENTIMENT CHO TOÃ€N Bá»˜ DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "analyzed_data = analyzer.analyze_dataframe(processed_data, text_column='cleaned_text')\n",
    "\n",
    "print(\"\\\\nâœ… HOÃ€N THÃ€NH PHÃ‚N TÃCH SENTIMENT!\")\n",
    "print(f\"ğŸ“Š Tá»•ng sá»‘ tweets Ä‘Ã£ phÃ¢n tÃ­ch: {len(analyzed_data)}\")\n",
    "\n",
    "# Hiá»ƒn thá»‹ thá»‘ng kÃª chi tiáº¿t\n",
    "print(\"\\\\nğŸ“ˆ THá»NG KÃŠ CHI TIáº¾T:\")\n",
    "\n",
    "# 1. PhÃ¢n bá»‘ sentiment tá»•ng thá»ƒ\n",
    "overall_sentiment = analyzed_data['sentiment_label'].value_counts()\n",
    "print(\"\\\\n1ï¸âƒ£ PhÃ¢n bá»‘ sentiment tá»•ng thá»ƒ:\")\n",
    "for sentiment, count in overall_sentiment.items():\n",
    "    percentage = (count / len(analyzed_data)) * 100\n",
    "    print(f\"   {sentiment}: {count} tweets ({percentage:.1f}%)\")\n",
    "\n",
    "# 2. PhÃ¢n bá»‘ sentiment theo tá»« khÃ³a\n",
    "print(\"\\\\n2ï¸âƒ£ PhÃ¢n bá»‘ sentiment theo tá»« khÃ³a:\")\n",
    "sentiment_by_keyword = analyzed_data.groupby(['keyword', 'sentiment_label']).size().unstack(fill_value=0)\n",
    "print(sentiment_by_keyword)\n",
    "\n",
    "# 3. Äiá»ƒm confidence trung bÃ¬nh\n",
    "print(\"\\\\n3ï¸âƒ£ Äiá»ƒm confidence trung bÃ¬nh:\")\n",
    "avg_confidence = analyzed_data.groupby('sentiment_label')['sentiment_score'].mean()\n",
    "for sentiment, score in avg_confidence.items():\n",
    "    print(f\"   {sentiment}: {score:.3f}\")\n",
    "\n",
    "# 4. Top tweets cho má»—i sentiment\n",
    "print(\"\\\\n4ï¸âƒ£ VÃ­ dá»¥ tweets cho má»—i sentiment:\")\n",
    "for sentiment in ['Positive', 'Negative', 'Neutral']:\n",
    "    if sentiment in analyzed_data['sentiment_label'].values:\n",
    "        sample = analyzed_data[analyzed_data['sentiment_label'] == sentiment].iloc[0]\n",
    "        print(f\"\\\\n   {sentiment.upper()}:\")\n",
    "        print(f\"   Original: {sample['text'][:100]}...\")\n",
    "        print(f\"   Cleaned:  {sample['cleaned_text'][:100]}...\")\n",
    "        print(f\"   Score:    {sample['sentiment_score']:.3f}\")\n",
    "        print(f\"   Keyword:  {sample['keyword']}\")\n",
    "\n",
    "# 5. LÆ°u káº¿t quáº£\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = f\"tweets_analyzed_{timestamp}.csv\"\n",
    "\n",
    "# Chá»n columns quan trá»ng Ä‘á»ƒ lÆ°u\n",
    "columns_to_save = [\n",
    "    'id', 'text', 'cleaned_text', 'keyword', 'created_at', 'username',\n",
    "    'sentiment_label', 'sentiment_score', 'positive_score', 'negative_score', 'neutral_score',\n",
    "    'retweet_count', 'like_count'\n",
    "]\n",
    "\n",
    "analyzed_data[columns_to_save].to_csv(output_file, index=False, encoding='utf-8')\n",
    "print(f\"\\\\nğŸ’¾ ÄÃ£ lÆ°u káº¿t quáº£ vÃ o file: {output_file}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ‰ PHÃ‚N TÃCH SENTIMENT HOÃ€N Táº¤T!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b39d2",
   "metadata": {},
   "source": [
    "# 6. PhÃ¢n tÃ­ch Káº¿t quáº£ vÃ  Trá»±c quan hÃ³a\n",
    "\n",
    "Táº¡o cÃ¡c biá»ƒu Ä‘á»“ tÆ°Æ¡ng tá»± nhÆ° Ä‘á»“ Ã¡n gá»‘c (Figures 6.4, 6.5) Ä‘á»ƒ so sÃ¡nh vÃ  phÃ¢n tÃ­ch káº¿t quáº£."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba9cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cáº¥u hÃ¬nh cho visualization\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Color mapping cho sentiments\n",
    "sentiment_colors = {\n",
    "    'Positive': '#2ca02c',    # Green\n",
    "    'Negative': '#d62728',    # Red  \n",
    "    'Neutral': '#ff7f0e'      # Orange\n",
    "}\n",
    "\n",
    "print(\"ğŸ¨ TRá»°C QUAN HÃ“A Káº¾T QUáº¢\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Biá»ƒu Ä‘á»“ cá»™t chá»“ng - Sentiment Distribution by Keyword (giá»‘ng Figure 6.4)\n",
    "print(\"\\\\n1ï¸âƒ£ Táº¡o biá»ƒu Ä‘á»“ phÃ¢n bá»‘ sentiment theo tá»« khÃ³a...\")\n",
    "\n",
    "# TÃ­nh tá»· lá»‡ pháº§n trÄƒm\n",
    "sentiment_by_keyword = analyzed_data.groupby(['keyword', 'sentiment_label']).size().unstack(fill_value=0)\n",
    "sentiment_percentages = sentiment_by_keyword.div(sentiment_by_keyword.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Táº¡o biá»ƒu Ä‘á»“\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "colors = [sentiment_colors.get(col, '#808080') for col in sentiment_percentages.columns]\n",
    "sentiment_percentages.plot(kind='bar', stacked=True, ax=ax, color=colors, width=0.7)\n",
    "\n",
    "ax.set_title('Sentiment Distribution by Keyword\\\\n(TÆ°Æ¡ng tá»± Figure 6.4 - Äá»“ Ã¡n gá»‘c)', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Keywords', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Percentage (%)', fontsize=12, fontweight='bold')\n",
    "ax.legend(title='Sentiment', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax.set_xticklabels(sentiment_percentages.index, rotation=45, ha='right')\n",
    "\n",
    "# ThÃªm labels trÃªn bars\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.1f%%', label_type='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Biá»ƒu Ä‘á»“ tá»•ng quan sentiment (giá»‘ng Figure 6.5)\n",
    "print(\"\\\\n2ï¸âƒ£ Táº¡o biá»ƒu Ä‘á»“ tá»•ng quan sentiment...\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Pie chart\n",
    "sentiment_counts = analyzed_data['sentiment_label'].value_counts()\n",
    "colors = [sentiment_colors.get(sentiment, '#808080') for sentiment in sentiment_counts.index]\n",
    "wedges, texts, autotexts = ax1.pie(sentiment_counts.values, labels=sentiment_counts.index, \n",
    "                                  autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "ax1.set_title('Overall Sentiment Distribution\\\\n(TÆ°Æ¡ng tá»± Figure 6.5)', fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "bars = ax2.bar(sentiment_counts.index, sentiment_counts.values, \n",
    "               color=[sentiment_colors.get(sentiment, '#808080') for sentiment in sentiment_counts.index],\n",
    "               alpha=0.8)\n",
    "ax2.set_title('Sentiment Counts', fontweight='bold')\n",
    "ax2.set_xlabel('Sentiment')\n",
    "ax2.set_ylabel('Number of Tweets')\n",
    "\n",
    "# ThÃªm labels trÃªn bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01*max(sentiment_counts),\n",
    "             f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Heatmap correlation giá»¯a keywords vÃ  sentiments\n",
    "print(\"\\\\n3ï¸âƒ£ Táº¡o heatmap correlation...\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Chuyá»ƒn vá» counts Ä‘á»ƒ dá»… Ä‘á»c\n",
    "heatmap_data = analyzed_data.groupby(['keyword', 'sentiment_label']).size().unstack(fill_value=0)\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='d', cmap='RdYlGn', center=heatmap_data.mean().mean())\n",
    "plt.title('Sentiment Counts by Keyword (Heatmap)', fontweight='bold', pad=20)\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Keywords')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Box plot cho sentiment scores\n",
    "print(\"\\\\n4ï¸âƒ£ Táº¡o box plot cho sentiment scores...\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "analyzed_data.boxplot(column='sentiment_score', by='sentiment_label', \n",
    "                     figsize=(12, 6), patch_artist=True)\n",
    "plt.title('Distribution of Confidence Scores by Sentiment')\n",
    "plt.xlabel('Sentiment Label')\n",
    "plt.ylabel('Confidence Score')\n",
    "plt.suptitle('')  # Remove automatic title\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\\\nâœ… HoÃ n thÃ nh táº¥t cáº£ biá»ƒu Ä‘á»“!\")\n",
    "\n",
    "# 6. Thá»‘ng kÃª nÃ¢ng cao\n",
    "print(\"\\\\n6ï¸âƒ£ Thá»‘ng kÃª nÃ¢ng cao:\")\n",
    "print(f\"   Average confidence score: {analyzed_data['sentiment_score'].mean():.3f}\")\n",
    "print(f\"   Tweets vá»›i high confidence (>0.8): {(analyzed_data['sentiment_score'] > 0.8).sum()} ({(analyzed_data['sentiment_score'] > 0.8).mean()*100:.1f}%)\")\n",
    "print(f\"   Tá»« khÃ³a cÃ³ sentiment tÃ­ch cá»±c nháº¥t: {sentiment_by_keyword.loc[:, 'Positive'].idxmax()}\")\n",
    "print(f\"   Tá»« khÃ³a cÃ³ sentiment tiÃªu cá»±c nháº¥t: {sentiment_by_keyword.loc[:, 'Negative'].idxmax()}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ¯ TRá»°C QUAN HÃ“A HOÃ€N Táº¤T!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fbd1dc",
   "metadata": {},
   "source": [
    "# 7. ÄÃ¡nh giÃ¡ Hiá»‡u suáº¥t \n",
    "\n",
    "PhÃ¢n tÃ­ch hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84369012",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“ˆ ÄÃNH GIÃ HIá»†U SUáº¤T\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. PhÃ¢n tÃ­ch performance metrics\n",
    "print(\"\\\\n1ï¸âƒ£ METRICS HIá»†U SUáº¤T:\")\n",
    "\n",
    "# Confidence score distribution\n",
    "confidence_stats = analyzed_data['sentiment_score'].describe()\n",
    "print(\"\\\\nğŸ“Š PhÃ¢n bá»‘ confidence scores:\")\n",
    "for stat, value in confidence_stats.items():\n",
    "    print(f\"   {stat}: {value:.4f}\")\n",
    "\n",
    "# High confidence predictions\n",
    "high_conf_threshold = 0.8\n",
    "high_conf_count = (analyzed_data['sentiment_score'] > high_conf_threshold).sum()\n",
    "high_conf_percentage = (high_conf_count / len(analyzed_data)) * 100\n",
    "\n",
    "print(f\"\\\\nğŸ¯ Predictions vá»›i high confidence (>{high_conf_threshold}):\")\n",
    "print(f\"   Count: {high_conf_count}/{len(analyzed_data)} ({high_conf_percentage:.1f}%)\")\n",
    "\n",
    "# Confidence by sentiment\n",
    "print(\"\\\\nğŸ“ˆ Average confidence by sentiment:\")\n",
    "conf_by_sentiment = analyzed_data.groupby('sentiment_label')['sentiment_score'].agg(['mean', 'std', 'count'])\n",
    "print(conf_by_sentiment.round(4))\n",
    "\n",
    "# 3. Insights tá»« káº¿t quáº£\n",
    "print(\"\\\\n\\\\n3ï¸âƒ£ INSIGHTS VÃ€ PHÃ‚N TÃCH:\")\n",
    "\n",
    "# Keyword analysis\n",
    "keyword_sentiment = analyzed_data.groupby('keyword')['sentiment_label'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "print(\"\\\\nğŸ“Š Sentiment ratios by keyword:\")\n",
    "print(keyword_sentiment.round(3))\n",
    "\n",
    "# Most positive/negative keywords\n",
    "if 'Positive' in keyword_sentiment.columns:\n",
    "    most_positive = keyword_sentiment['Positive'].idxmax()\n",
    "    print(f\"\\\\nğŸ˜Š Most positive keyword: {most_positive} ({keyword_sentiment['Positive'][most_positive]:.1%} positive)\")\n",
    "\n",
    "if 'Negative' in keyword_sentiment.columns:\n",
    "    most_negative = keyword_sentiment['Negative'].idxmax()\n",
    "    print(f\"ğŸ˜ Most negative keyword: {most_negative} ({keyword_sentiment['Negative'][most_negative]:.1%} negative)\")\n",
    "\n",
    "# 4. Táº¡o summary report\n",
    "print(\"\\\\n\\\\n4ï¸âƒ£ SUMMARY REPORT:\")\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¯ TWITTER SENTIMENT ANALYSIS - FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Dataset info\n",
    "print(f\"\\\\nğŸ“Š DATASET INFO:\")\n",
    "print(f\"   Total tweets analyzed: {len(analyzed_data)}\")\n",
    "print(f\"   Keywords: {', '.join(KEYWORDS)}\")\n",
    "print(f\"   Date range: {analyzed_data['created_at'].min().date()} to {analyzed_data['created_at'].max().date()}\")\n",
    "\n",
    "# Model info\n",
    "print(f\"\\\\nğŸ¤– MODEL INFO:\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Device: {'GPU' if analyzer.device == 0 else 'CPU'}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Results summary\n",
    "print(f\"\\\\nğŸ“ˆ RESULTS SUMMARY:\")\n",
    "overall_sentiment = analyzed_data['sentiment_label'].value_counts(normalize=True) * 100\n",
    "for sentiment, percentage in overall_sentiment.items():\n",
    "    print(f\"   {sentiment}: {percentage:.1f}%\")\n",
    "\n",
    "print(f\"\\\\nâ­ QUALITY METRICS:\")\n",
    "print(f\"   Average confidence: {analyzed_data['sentiment_score'].mean():.3f}\")\n",
    "print(f\"   High confidence predictions: {high_conf_percentage:.1f}%\")\n",
    "print(f\"   Processing time: ~{len(analyzed_data) / 60:.1f} tweets/minute\")\n",
    "\n",
    "# 5. Recommendations for improvement\n",
    "print(\"\\\\n\\\\n5ï¸âƒ£ KHUYáº¾N NGHá»Š Cáº¢I TIáº¾N:\")\n",
    "print(\"\\\\nğŸ”§ Technical improvements:\")\n",
    "print(\"   â€¢ ThÃªm data validation vÃ  error handling\")\n",
    "print(\"   â€¢ Implement caching cho model predictions\")\n",
    "print(\"   â€¢ Sá»­ dá»¥ng GPU Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™ xá»­ lÃ½\")\n",
    "print(\"   â€¢ ThÃªm real-time streaming capabilities\")\n",
    "\n",
    "print(\"\\\\nğŸ“Š Analysis improvements:\")\n",
    "print(\"   â€¢ Thu tháº­p ground truth data Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ accuracy\")\n",
    "print(\"   â€¢ ThÃªm temporal analysis (xu hÆ°á»›ng theo thá»i gian)\")\n",
    "print(\"   â€¢ PhÃ¢n tÃ­ch deeper insights (hashtags, mentions)\")\n",
    "print(\"   â€¢ A/B testing vá»›i cÃ¡c models khÃ¡c\")\n",
    "\n",
    "print(\"\\\\nğŸš€ Scaling improvements:\")\n",
    "print(\"   â€¢ Containerization vá»›i Docker\")\n",
    "print(\"   â€¢ Deploy lÃªn cloud (AWS/Azure)\")\n",
    "print(\"   â€¢ Implement proper logging vÃ  monitoring\")\n",
    "print(\"   â€¢ Táº¡o web dashboard vá»›i Streamlit\")\n",
    "\n",
    "# 6. Export final results\n",
    "final_results = {\n",
    "    'total_tweets': len(analyzed_data),\n",
    "    'sentiment_distribution': analyzed_data['sentiment_label'].value_counts().to_dict(),\n",
    "    'sentiment_percentages': (analyzed_data['sentiment_label'].value_counts(normalize=True) * 100).round(2).to_dict(),\n",
    "    'average_confidence': float(analyzed_data['sentiment_score'].mean()),\n",
    "    'high_confidence_rate': float(high_conf_percentage),\n",
    "    'keywords_analyzed': KEYWORDS,\n",
    "    'model_used': MODEL_NAME,\n",
    "    'processing_timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "import json\n",
    "summary_file = f\"analysis_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\\\nğŸ’¾ Final summary saved to: {summary_file}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*70)\n",
    "print(\"ğŸ‰ PHÃ‚N TÃCH HOÃ€N Táº¤T\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Display success message\n",
    "print(\"\\\\nğŸ† THÃ€NH CÃ”NG:\")\n",
    "print(\"   âœ… Thu tháº­p dá»¯ liá»‡u tá»« Twitter (hoáº·c táº¡o dá»¯ liá»‡u máº«u)\")\n",
    "print(\"   âœ… Tiá»n xá»­ lÃ½ vÄƒn báº£n theo Ä‘Ãºng pipeline Ä‘á»“ Ã¡n gá»‘c\")\n",
    "print(\"   âœ… Ãp dá»¥ng mÃ´ hÃ¬nh RoBERTa-Twitter tá»« Hugging Face\")\n",
    "print(\"   âœ… ÄÆ¡n giáº£n hÃ³a kiáº¿n trÃºc nhÆ°ng váº«n Ä‘áº£m báº£o cháº¥t lÆ°á»£ng\")\n",
    "\n",
    "print(\"\\\\nğŸ¯ Dá»± Ã¡n Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ trÃ¬nh bÃ y vÃ  bÃ¡o cÃ¡o!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
