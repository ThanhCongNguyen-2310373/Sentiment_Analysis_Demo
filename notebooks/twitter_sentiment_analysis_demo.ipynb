{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daf9fad9",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis với RoBERTa Model\n",
    "\n",
    "## Tái hiện đồ án NLP phân tích cảm xúc với kiến trúc đơn giản hóa\n",
    "\n",
    "**Dự án này tái hiện và mở rộng nghiên cứu về phân tích cảm xúc Twitter sử dụng mô hình RoBERTa-Twitter, dựa trên đồ án gốc của anh Thịnh Lâm Tấn.**\n",
    "\n",
    "### 🎯 **Mục tiêu**\n",
    "- Tái hiện mô hình AI phân tích cảm xúc với dữ liệu mới\n",
    "- Sử dụng RoBERTa-Twitter model từ Hugging Face\n",
    "- Đơn giản hóa kiến trúc: Python + Pandas + Transformers (thay vì Kafka + Spark + MongoDB)\n",
    "- Tạo trực quan hóa tương tự đồ án gốc\n",
    "\n",
    "### 🔄 **Quy trình**\n",
    "```\n",
    "Crawl Data → Text Preprocessing → RoBERTa Analysis → Visualization → Report\n",
    "```\n",
    "\n",
    "### 📊 **So sánh kiến trúc**\n",
    "| Đồ án gốc | Dự án hiện tại |\n",
    "|-----------|----------------|\n",
    "| Producer → Kafka → Spark → MongoDB | Python Script → CSV → RoBERTa → Charts |\n",
    "| Big Data Architecture | Simplified Pipeline |\n",
    "| Distributed Processing | Single Machine |\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook này sẽ hướng dẫn chi tiết từng bước để xây dựng hệ thống phân tích cảm xúc hoàn chỉnh.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89be4a5",
   "metadata": {},
   "source": [
    "# 1. Thiết lập Môi trường và Cài đặt Thư viện\n",
    "\n",
    "Đầu tiên, chúng ta sẽ cài đặt và import các thư viện cần thiết cho dự án."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f54babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cài đặt các thư viện cần thiết (chạy lần đầu)\n",
    "# !pip install pandas requests transformers torch matplotlib seaborn plotly tqdm python-dotenv\n",
    "\n",
    "# Import các thư viện chính\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Any\n",
    "\n",
    "# Thư viện cho machine learning và NLP\n",
    "try:\n",
    "    from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "    import torch\n",
    "    print(\"✅ Transformers và PyTorch đã được cài đặt\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "except ImportError as e:\n",
    "    print(\"❌ Lỗi import transformers/torch:\", e)\n",
    "    print(\"Vui lòng chạy: !pip install transformers torch\")\n",
    "\n",
    "# Thư viện cho visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Thiết lập style cho plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"🚀 THIẾT LẬP HOÀN TẤT\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c979e2d9",
   "metadata": {},
   "source": [
    "# 2. Thu thập Dữ liệu từ Twitter API\n",
    "\n",
    "Chúng ta sẽ sử dụng twitterapi.io để thu thập tweets với các từ khóa AI phổ biến, tương tự như đồ án gốc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beceab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cấu hình API và từ khóa (giống đồ án gốc + thêm từ khóa mới)\n",
    "TWITTER_API_KEY = \"your_api_key_here\"  # Thay thế bằng API key thực từ twitterapi.io\n",
    "TWITTER_API_BASE_URL = \"https://api.twitterapi.io/v1\"\n",
    "\n",
    "# Từ khóa từ đồ án gốc + từ khóa mới\n",
    "KEYWORDS = [\n",
    "    \"GPT\", \"Copilot\", \"Gemini\",      # Từ đồ án gốc của anh Thịnh\n",
    "    \"GPT-4o\", \"Sora\", \"Llama 3\",     # Từ khóa AI mới\n",
    "    \"Claude\", \"ChatGPT\"              # Bổ sung thêm\n",
    "]\n",
    "\n",
    "MAX_TWEETS_PER_KEYWORD = 100  # Giảm để demo nhanh\n",
    "RATE_LIMIT_DELAY = 2\n",
    "\n",
    "print(\"📋 Cấu hình thu thập dữ liệu:\")\n",
    "print(f\"   Keywords: {KEYWORDS}\")\n",
    "print(f\"   Max tweets per keyword: {MAX_TWEETS_PER_KEYWORD}\")\n",
    "print(f\"   Total expected tweets: {len(KEYWORDS) * MAX_TWEETS_PER_KEYWORD}\")\n",
    "\n",
    "class TwitterCrawler:\n",
    "    \"\"\"\n",
    "    Twitter data crawler sử dụng twitterapi.io\n",
    "    Tương tự đồ án gốc nhưng đơn giản hóa\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = TWITTER_API_BASE_URL\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'Authorization': f'Bearer {api_key}',\n",
    "            'Content-Type': 'application/json'\n",
    "        })\n",
    "    \n",
    "    def search_tweets(self, query: str, max_results: int = 100) -> List[Dict]:\n",
    "        \"\"\"Thu thập tweets theo từ khóa\"\"\"\n",
    "        endpoint = f\"{self.base_url}/search\"\n",
    "        \n",
    "        params = {\n",
    "            'query': query,\n",
    "            'max_results': min(max_results, 100),\n",
    "            'tweet.fields': 'created_at,author_id,public_metrics,lang',\n",
    "            'user.fields': 'name,username,verified',\n",
    "            'expansions': 'author_id'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(endpoint, params=params)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            tweets = []\n",
    "            \n",
    "            if 'data' in data:\n",
    "                # Tạo mapping user info\n",
    "                users = {}\n",
    "                if 'includes' in data and 'users' in data['includes']:\n",
    "                    users = {user['id']: user for user in data['includes']['users']}\n",
    "                \n",
    "                for tweet in data['data']:\n",
    "                    user_info = users.get(tweet.get('author_id', ''), {})\n",
    "                    \n",
    "                    tweet_data = {\n",
    "                        'id': tweet.get('id', ''),\n",
    "                        'text': tweet.get('text', ''),\n",
    "                        'created_at': tweet.get('created_at', ''),\n",
    "                        'username': user_info.get('username', ''),\n",
    "                        'user_name': user_info.get('name', ''),\n",
    "                        'retweet_count': tweet.get('public_metrics', {}).get('retweet_count', 0),\n",
    "                        'like_count': tweet.get('public_metrics', {}).get('like_count', 0),\n",
    "                        'lang': tweet.get('lang', ''),\n",
    "                        'keyword': query\n",
    "                    }\n",
    "                    tweets.append(tweet_data)\n",
    "            \n",
    "            return tweets\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"❌ Lỗi API cho từ khóa '{query}': {e}\")\n",
    "            return []\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"❌ Lỗi parse JSON cho từ khóa '{query}': {e}\")\n",
    "            return []\n",
    "    \n",
    "    def crawl_multiple_keywords(self, keywords: List[str], max_tweets: int) -> pd.DataFrame:\n",
    "        \"\"\"Thu thập tweets cho nhiều từ khóa\"\"\"\n",
    "        all_tweets = []\n",
    "        \n",
    "        print(f\"🔄 Bắt đầu thu thập tweets cho {len(keywords)} từ khóa...\")\n",
    "        \n",
    "        for i, keyword in enumerate(keywords, 1):\n",
    "            print(f\"   [{i}/{len(keywords)}] Đang crawl: '{keyword}'\")\n",
    "            \n",
    "            tweets = self.search_tweets(keyword, max_tweets)\n",
    "            \n",
    "            if tweets:\n",
    "                all_tweets.extend(tweets)\n",
    "                print(f\"      ✅ Thu thập được {len(tweets)} tweets\")\n",
    "            else:\n",
    "                print(f\"      ⚠️ Không thu thập được tweets nào\")\n",
    "            \n",
    "            # Rate limiting\n",
    "            if i < len(keywords):  # Không delay ở lần cuối\n",
    "                time.sleep(RATE_LIMIT_DELAY)\n",
    "        \n",
    "        # Chuyển thành DataFrame\n",
    "        df = pd.DataFrame(all_tweets)\n",
    "        \n",
    "        if not df.empty:\n",
    "            # Xử lý dữ liệu\n",
    "            df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "            df = df.drop_duplicates(subset=['id'])  # Loại bỏ duplicates\n",
    "            df = df[df['lang'] == 'en']  # Chỉ lấy tweets tiếng Anh\n",
    "            \n",
    "            print(f\"✅ Hoàn thành! Tổng cộng {len(df)} tweets unique (tiếng Anh)\")\n",
    "        else:\n",
    "            print(\"❌ Không thu thập được dữ liệu nào\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Tạo crawler instance \n",
    "print(\"🤖 Khởi tạo Twitter Crawler...\")\n",
    "if TWITTER_API_KEY == \"your_api_key_here\":\n",
    "    print(\"⚠️ CẢNH BÁO: Bạn cần thay thế TWITTER_API_KEY bằng API key thực\")\n",
    "    print(\"   Để demo, chúng ta sẽ tạo dữ liệu mẫu...\")\n",
    "    use_real_api = False\n",
    "else:\n",
    "    crawler = TwitterCrawler(TWITTER_API_KEY)\n",
    "    use_real_api = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f104fdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thu thập dữ liệu thực tế hoặc tạo dữ liệu mẫu để demo\n",
    "if use_real_api:\n",
    "    # Sử dụng API thực\n",
    "    print(\"🔄 Thu thập dữ liệu từ Twitter API...\")\n",
    "    raw_data = crawler.crawl_multiple_keywords(KEYWORDS, MAX_TWEETS_PER_KEYWORD)\n",
    "else:\n",
    "    # Tạo dữ liệu mẫu để demo\n",
    "    print(\"🔧 Tạo dữ liệu mẫu để demo...\")\n",
    "    \n",
    "    sample_tweets = [\n",
    "        \"I love using GPT-4! It's amazing for coding assistance and problem solving.\",\n",
    "        \"ChatGPT is helpful but sometimes gives wrong information. Need to be careful.\",\n",
    "        \"GitHub Copilot saves me so much time when programming. Highly recommend!\",\n",
    "        \"Gemini is decent but I still prefer GPT for most tasks.\",\n",
    "        \"The new GPT-4o model is incredibly fast and accurate. Impressed!\",\n",
    "        \"Sora AI video generation is mind-blowing. The future is here!\",\n",
    "        \"Llama 3 open source model is surprisingly good for a free alternative.\",\n",
    "        \"Claude is great for writing and analysis tasks. Very thoughtful responses.\",\n",
    "        \"AI copilots in coding are game changers. Can't imagine coding without them now.\",\n",
    "        \"These AI tools are making everyone more productive. Exciting times!\",\n",
    "        \"GPT sometimes hallucinates facts. Always double-check important information.\",\n",
    "        \"Copilot suggestions are usually good but sometimes completely off-topic.\",\n",
    "        \"I'm worried about AI replacing human creativity and jobs.\",\n",
    "        \"The quality of AI responses keeps getting better every month.\",\n",
    "        \"Using multiple AI tools together gives the best results for complex tasks.\"\n",
    "    ] * 10  # Repeat để có đủ dữ liệu\n",
    "    \n",
    "    # Tạo DataFrame mẫu\n",
    "    import random\n",
    "    \n",
    "    raw_data = []\n",
    "    for i, text in enumerate(sample_tweets):\n",
    "        tweet_data = {\n",
    "            'id': f'tweet_{i}',\n",
    "            'text': text,\n",
    "            'created_at': pd.Timestamp.now() - pd.Timedelta(days=random.randint(0, 30)),\n",
    "            'username': f'user_{i % 20}',\n",
    "            'user_name': f'User {i % 20}',\n",
    "            'retweet_count': random.randint(0, 100),\n",
    "            'like_count': random.randint(0, 500),\n",
    "            'lang': 'en',\n",
    "            'keyword': random.choice(KEYWORDS)\n",
    "        }\n",
    "        raw_data.append(tweet_data)\n",
    "    \n",
    "    raw_data = pd.DataFrame(raw_data)\n",
    "    print(f\"✅ Tạo thành công {len(raw_data)} tweets mẫu\")\n",
    "\n",
    "# Hiển thị thông tin dữ liệu thu thập được\n",
    "print(\"\\\\n📊 THÔNG TIN DỮ LIỆU THU THẬP:\")\n",
    "print(f\"   Tổng số tweets: {len(raw_data)}\")\n",
    "print(f\"   Khoảng thời gian: {raw_data['created_at'].min()} đến {raw_data['created_at'].max()}\")\n",
    "\n",
    "# Phân bố theo từ khóa\n",
    "keyword_distribution = raw_data['keyword'].value_counts()\n",
    "print(\"\\\\n📈 Phân bố theo từ khóa:\")\n",
    "for keyword, count in keyword_distribution.items():\n",
    "    print(f\"   {keyword}: {count} tweets\")\n",
    "\n",
    "# Hiển thị mẫu dữ liệu\n",
    "print(\"\\\\n🔍 Mẫu dữ liệu:\")\n",
    "print(raw_data[['text', 'username', 'keyword', 'created_at']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d0b42",
   "metadata": {},
   "source": [
    "# 3. Tiền xử lý và Làm sạch Văn bản\n",
    "\n",
    "Áp dụng các bước tiền xử lý giống như đồ án gốc: chuyển chữ thường, loại bỏ URL, mentions, hashtags, ký tự đặc biệt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c26b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Text preprocessing class theo đồ án gốc\n",
    "    Áp dụng các bước làm sạch tương tự SQLTransformer trong Spark\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cleaning_stats = {\n",
    "            'urls_removed': 0,\n",
    "            'mentions_removed': 0,\n",
    "            'hashtags_removed': 0,\n",
    "            'texts_too_short': 0\n",
    "        }\n",
    "    \n",
    "    def remove_urls(self, text: str) -> str:\n",
    "        \"\"\"Loại bỏ URLs\"\"\"\n",
    "        url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        if url_pattern.search(text):\n",
    "            self.cleaning_stats['urls_removed'] += 1\n",
    "        return url_pattern.sub('', text)\n",
    "    \n",
    "    def remove_mentions(self, text: str) -> str:\n",
    "        \"\"\"Loại bỏ @mentions\"\"\"\n",
    "        mention_pattern = re.compile(r'@[\\\\w_]+')\n",
    "        if mention_pattern.search(text):\n",
    "            self.cleaning_stats['mentions_removed'] += 1\n",
    "        return mention_pattern.sub('', text)\n",
    "    \n",
    "    def remove_hashtags(self, text: str) -> str:\n",
    "        \"\"\"Loại bỏ hashtags\"\"\"\n",
    "        hashtag_pattern = re.compile(r'#[\\\\w_]+')\n",
    "        if hashtag_pattern.search(text):\n",
    "            self.cleaning_stats['hashtags_removed'] += 1\n",
    "        return hashtag_pattern.sub('', text)\n",
    "    \n",
    "    def remove_special_chars(self, text: str) -> str:\n",
    "        \"\"\"Loại bỏ ký tự đặc biệt, chỉ giữ chữ cái và số\"\"\"\n",
    "        return re.sub(r'[^a-zA-Z0-9\\\\s]', '', text)\n",
    "    \n",
    "    def remove_extra_whitespace(self, text: str) -> str:\n",
    "        \"\"\"Loại bỏ khoảng trắng thừa\"\"\"\n",
    "        return re.sub(r'\\\\s+', ' ', text).strip()\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Áp dụng tất cả các bước làm sạch\n",
    "        Theo thứ tự giống đồ án gốc\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return \"\"\n",
    "        \n",
    "        # 1. Chuyển chữ thường (giống đồ án gốc)\n",
    "        text = text.lower()\n",
    "        \n",
    "        # 2. Loại bỏ URLs\n",
    "        text = self.remove_urls(text)\n",
    "        \n",
    "        # 3. Loại bỏ mentions và hashtags\n",
    "        text = self.remove_mentions(text)\n",
    "        text = self.remove_hashtags(text)\n",
    "        \n",
    "        # 4. Loại bỏ ký tự đặc biệt\n",
    "        text = self.remove_special_chars(text)\n",
    "        \n",
    "        # 5. Loại bỏ khoảng trắng thừa\n",
    "        text = self.remove_extra_whitespace(text)\n",
    "        \n",
    "        # 6. Kiểm tra độ dài tối thiểu\n",
    "        if len(text) < 10:\n",
    "            self.cleaning_stats['texts_too_short'] += 1\n",
    "            return \"\"\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def preprocess_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Xử lý toàn bộ DataFrame\"\"\"\n",
    "        print(\"🧹 Bắt đầu tiền xử lý văn bản...\")\n",
    "        \n",
    "        # Reset stats\n",
    "        self.cleaning_stats = {key: 0 for key in self.cleaning_stats}\n",
    "        \n",
    "        # Tạo copy để không thay đổi dữ liệu gốc\n",
    "        processed_df = df.copy()\n",
    "        \n",
    "        # Áp dụng làm sạch\n",
    "        processed_df['cleaned_text'] = processed_df['text'].apply(self.clean_text)\n",
    "        \n",
    "        # Loại bỏ tweets rỗng sau khi làm sạch\n",
    "        original_count = len(processed_df)\n",
    "        processed_df = processed_df[processed_df['cleaned_text'].str.len() > 0]\n",
    "        removed_count = original_count - len(processed_df)\n",
    "        \n",
    "        # Thêm metadata\n",
    "        processed_df['original_length'] = df['text'].str.len()\n",
    "        processed_df['cleaned_length'] = processed_df['cleaned_text'].str.len()\n",
    "        processed_df['preprocessing_timestamp'] = pd.Timestamp.now()\n",
    "        \n",
    "        # In thống kê\n",
    "        print(f\"   ✅ Xử lý hoàn thành:\")\n",
    "        print(f\"      - URLs loại bỏ: {self.cleaning_stats['urls_removed']}\")\n",
    "        print(f\"      - Mentions loại bỏ: {self.cleaning_stats['mentions_removed']}\")\n",
    "        print(f\"      - Hashtags loại bỏ: {self.cleaning_stats['hashtags_removed']}\")\n",
    "        print(f\"      - Texts quá ngắn: {self.cleaning_stats['texts_too_short']}\")\n",
    "        print(f\"      - Tweets loại bỏ: {removed_count}\")\n",
    "        print(f\"      - Tweets còn lại: {len(processed_df)}\")\n",
    "        \n",
    "        return processed_df\n",
    "\n",
    "# Khởi tạo preprocessor và xử lý dữ liệu\n",
    "preprocessor = TextPreprocessor()\n",
    "processed_data = preprocessor.preprocess_dataframe(raw_data)\n",
    "\n",
    "# Hiển thị ví dụ trước và sau khi làm sạch\n",
    "print(\"\\\\n🔍 VÍ DỤ TRƯỚC VÀ SAU KHI LÀM SẠCH:\")\n",
    "sample_indices = [0, 5, 10]\n",
    "for i in sample_indices:\n",
    "    if i < len(processed_data):\n",
    "        original = raw_data.iloc[i]['text']\n",
    "        cleaned = processed_data.iloc[i]['cleaned_text']\n",
    "        print(f\"\\\\n[{i+1}] Gốc: {original}\")\n",
    "        print(f\"    Sạch: {cleaned}\")\n",
    "\n",
    "# Thống kê độ dài văn bản\n",
    "print(\"\\\\n📏 THỐNG KÊ ĐỘ DÀI VĂN BẢN:\")\n",
    "print(f\"   Độ dài trung bình (gốc): {processed_data['original_length'].mean():.1f} ký tự\")\n",
    "print(f\"   Độ dài trung bình (sạch): {processed_data['cleaned_length'].mean():.1f} ký tự\")\n",
    "print(f\"   Giảm: {((processed_data['original_length'].mean() - processed_data['cleaned_length'].mean()) / processed_data['original_length'].mean() * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f846c6",
   "metadata": {},
   "source": [
    "# 4. Tải và Cấu hình Mô hình RoBERTa\n",
    "\n",
    "Sử dụng mô hình `cardiffnlp/twitter-roberta-base-sentiment-latest` từ Hugging Face, giống như đồ án gốc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f073d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cấu hình mô hình RoBERTa\n",
    "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "BATCH_SIZE = 16  # Giảm để tránh memory issues\n",
    "\n",
    "# Label mapping cho RoBERTa (theo đồ án gốc)\n",
    "SENTIMENT_LABEL_MAPPING = {\n",
    "    'LABEL_0': 'Negative',\n",
    "    'LABEL_1': 'Neutral', \n",
    "    'LABEL_2': 'Positive'\n",
    "}\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    Sentiment Analysis sử dụng RoBERTa-Twitter\n",
    "    Tương tự Pandas UDF approach trong đồ án gốc nhưng đơn giản hóa\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = MODEL_NAME, batch_size: int = BATCH_SIZE):\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.pipeline = None\n",
    "        self.device = None\n",
    "        \n",
    "        self._setup_model()\n",
    "    \n",
    "    def _setup_model(self):\n",
    "        \"\"\"Khởi tạo sentiment analysis pipeline\"\"\"\n",
    "        # Kiểm tra GPU\n",
    "        self.device = 0 if torch.cuda.is_available() else -1\n",
    "        device_name = \"GPU\" if self.device == 0 else \"CPU\"\n",
    "        \n",
    "        print(f\"🤖 Đang tải mô hình RoBERTa: {self.model_name}\")\n",
    "        print(f\"   Device: {device_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Tải pipeline (giống đồ án gốc)\n",
    "            self.pipeline = pipeline(\n",
    "                \"sentiment-analysis\",\n",
    "                model=self.model_name,\n",
    "                tokenizer=self.model_name,\n",
    "                device=self.device,\n",
    "                return_all_scores=True  # Lấy scores cho tất cả labels\n",
    "            )\n",
    "            \n",
    "            print(\"   ✅ Mô hình đã được tải thành công!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Lỗi tải mô hình: {e}\")\n",
    "            print(\"   🔄 Đang thử lại với CPU...\")\n",
    "            \n",
    "            # Fallback to CPU\n",
    "            self.device = -1\n",
    "            self.pipeline = pipeline(\n",
    "                \"sentiment-analysis\",\n",
    "                model=self.model_name,\n",
    "                tokenizer=self.model_name,\n",
    "                device=self.device,\n",
    "                return_all_scores=True\n",
    "            )\n",
    "            print(\"   ✅ Mô hình đã được tải trên CPU!\")\n",
    "    \n",
    "    def _process_predictions(self, predictions: List[List[Dict]]) -> List[Dict]:\n",
    "        \"\"\"Xử lý kết quả dự đoán thành format dễ đọc\"\"\"\n",
    "        processed_results = []\n",
    "        \n",
    "        for pred_list in predictions:\n",
    "            # Tìm prediction có score cao nhất\n",
    "            best_pred = max(pred_list, key=lambda x: x['score'])\n",
    "            \n",
    "            # Map label sang định dạng dễ đọc\n",
    "            raw_label = best_pred['label']\n",
    "            readable_label = SENTIMENT_LABEL_MAPPING.get(raw_label, raw_label)\n",
    "            \n",
    "            # Tạo result dictionary\n",
    "            result = {\n",
    "                'sentiment_label': readable_label,\n",
    "                'sentiment_score': best_pred['score'],\n",
    "                'raw_label': raw_label,\n",
    "                'all_scores': {\n",
    "                    SENTIMENT_LABEL_MAPPING.get(item['label'], item['label']): item['score'] \n",
    "                    for item in pred_list\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            processed_results.append(result)\n",
    "        \n",
    "        return processed_results\n",
    "    \n",
    "    def predict_sentiment(self, texts: List[str]) -> List[Dict]:\n",
    "        \"\"\"Dự đoán sentiment cho list texts với batch processing\"\"\"\n",
    "        if not texts:\n",
    "            return []\n",
    "        \n",
    "        # Lọc texts hợp lệ\n",
    "        valid_texts = [text for text in texts if text and text.strip()]\n",
    "        \n",
    "        if not valid_texts:\n",
    "            print(\"⚠️ Không có texts hợp lệ để phân tích\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"🔄 Đang phân tích sentiment cho {len(valid_texts)} texts...\")\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Xử lý theo batch (giống đồ án gốc)\n",
    "        from tqdm import tqdm\n",
    "        \n",
    "        for i in tqdm(range(0, len(valid_texts), self.batch_size), desc=\"Processing batches\"):\n",
    "            batch_texts = valid_texts[i:i + self.batch_size]\n",
    "            \n",
    "            try:\n",
    "                # Gọi model\n",
    "                batch_predictions = self.pipeline(batch_texts)\n",
    "                \n",
    "                # Xử lý kết quả\n",
    "                batch_results = self._process_predictions(batch_predictions)\n",
    "                results.extend(batch_results)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Lỗi xử lý batch {i//self.batch_size + 1}: {e}\")\n",
    "                # Thêm kết quả trống cho batch bị lỗi\n",
    "                empty_results = [{\n",
    "                    'sentiment_label': 'Unknown',\n",
    "                    'sentiment_score': 0.0,\n",
    "                    'raw_label': 'ERROR',\n",
    "                    'all_scores': {}\n",
    "                }] * len(batch_texts)\n",
    "                results.extend(empty_results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_dataframe(self, df: pd.DataFrame, text_column: str = 'cleaned_text') -> pd.DataFrame:\n",
    "        \"\"\"Phân tích sentiment cho DataFrame (giống Spark DataFrame processing)\"\"\"\n",
    "        if df.empty:\n",
    "            print(\"⚠️ DataFrame rỗng\")\n",
    "            return df\n",
    "        \n",
    "        if text_column not in df.columns:\n",
    "            print(f\"❌ Không tìm thấy column '{text_column}'\")\n",
    "            return df\n",
    "        \n",
    "        print(f\"🚀 Bắt đầu phân tích sentiment cho {len(df)} tweets...\")\n",
    "        \n",
    "        # Tạo copy\n",
    "        result_df = df.copy()\n",
    "        \n",
    "        # Lấy texts để phân tích\n",
    "        texts = df[text_column].fillna('').astype(str).tolist()\n",
    "        \n",
    "        # Thực hiện phân tích\n",
    "        predictions = self.predict_sentiment(texts)\n",
    "        \n",
    "        if predictions:\n",
    "            # Thêm kết quả vào DataFrame\n",
    "            result_df['sentiment_label'] = [pred['sentiment_label'] for pred in predictions]\n",
    "            result_df['sentiment_score'] = [pred['sentiment_score'] for pred in predictions]\n",
    "            result_df['raw_sentiment_label'] = [pred['raw_label'] for pred in predictions]\n",
    "            \n",
    "            # Thêm scores riêng lẻ\n",
    "            result_df['positive_score'] = [pred['all_scores'].get('Positive', 0.0) for pred in predictions]\n",
    "            result_df['negative_score'] = [pred['all_scores'].get('Negative', 0.0) for pred in predictions]\n",
    "            result_df['neutral_score'] = [pred['all_scores'].get('Neutral', 0.0) for pred in predictions]\n",
    "            \n",
    "            # Metadata\n",
    "            result_df['analysis_timestamp'] = pd.Timestamp.now()\n",
    "            result_df['model_used'] = self.model_name\n",
    "            \n",
    "            print(\"✅ Phân tích hoàn thành!\")\n",
    "            \n",
    "            # Thống kê kết quả\n",
    "            sentiment_counts = result_df['sentiment_label'].value_counts()\n",
    "            print(\"\\\\n📊 Phân bố sentiment:\")\n",
    "            for sentiment, count in sentiment_counts.items():\n",
    "                percentage = (count / len(result_df)) * 100\n",
    "                print(f\"   {sentiment}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        else:\n",
    "            print(\"❌ Không có kết quả dự đoán\")\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "# Khởi tạo analyzer\n",
    "print(\"🤖 Khởi tạo Sentiment Analyzer...\")\n",
    "analyzer = SentimentAnalyzer()\n",
    "\n",
    "# Test với một vài câu mẫu trước\n",
    "test_texts = [\n",
    "    \"i love using gpt its amazing for coding\",\n",
    "    \"chatgpt is helpful but sometimes wrong\",\n",
    "    \"github copilot saves me time programming\"\n",
    "]\n",
    "\n",
    "print(\"\\\\n🧪 Test với câu mẫu:\")\n",
    "test_results = analyzer.predict_sentiment(test_texts)\n",
    "for text, result in zip(test_texts, test_results):\n",
    "    print(f\"   Text: {text}\")\n",
    "    print(f\"   → {result['sentiment_label']} ({result['sentiment_score']:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049b886a",
   "metadata": {},
   "source": [
    "# 5. Thực hiện Phân tích Cảm xúc\n",
    "\n",
    "Áp dụng mô hình RoBERTa để dự đoán sentiment cho toàn bộ dataset đã được tiền xử lý."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9a1ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thực hiện phân tích sentiment cho toàn bộ dataset\n",
    "print(\"🚀 BẮT ĐẦU PHÂN TÍCH SENTIMENT CHO TOÀN BỘ DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "analyzed_data = analyzer.analyze_dataframe(processed_data, text_column='cleaned_text')\n",
    "\n",
    "print(\"\\\\n✅ HOÀN THÀNH PHÂN TÍCH SENTIMENT!\")\n",
    "print(f\"📊 Tổng số tweets đã phân tích: {len(analyzed_data)}\")\n",
    "\n",
    "# Hiển thị thống kê chi tiết\n",
    "print(\"\\\\n📈 THỐNG KÊ CHI TIẾT:\")\n",
    "\n",
    "# 1. Phân bố sentiment tổng thể\n",
    "overall_sentiment = analyzed_data['sentiment_label'].value_counts()\n",
    "print(\"\\\\n1️⃣ Phân bố sentiment tổng thể:\")\n",
    "for sentiment, count in overall_sentiment.items():\n",
    "    percentage = (count / len(analyzed_data)) * 100\n",
    "    print(f\"   {sentiment}: {count} tweets ({percentage:.1f}%)\")\n",
    "\n",
    "# 2. Phân bố sentiment theo từ khóa\n",
    "print(\"\\\\n2️⃣ Phân bố sentiment theo từ khóa:\")\n",
    "sentiment_by_keyword = analyzed_data.groupby(['keyword', 'sentiment_label']).size().unstack(fill_value=0)\n",
    "print(sentiment_by_keyword)\n",
    "\n",
    "# 3. Điểm confidence trung bình\n",
    "print(\"\\\\n3️⃣ Điểm confidence trung bình:\")\n",
    "avg_confidence = analyzed_data.groupby('sentiment_label')['sentiment_score'].mean()\n",
    "for sentiment, score in avg_confidence.items():\n",
    "    print(f\"   {sentiment}: {score:.3f}\")\n",
    "\n",
    "# 4. Top tweets cho mỗi sentiment\n",
    "print(\"\\\\n4️⃣ Ví dụ tweets cho mỗi sentiment:\")\n",
    "for sentiment in ['Positive', 'Negative', 'Neutral']:\n",
    "    if sentiment in analyzed_data['sentiment_label'].values:\n",
    "        sample = analyzed_data[analyzed_data['sentiment_label'] == sentiment].iloc[0]\n",
    "        print(f\"\\\\n   {sentiment.upper()}:\")\n",
    "        print(f\"   Original: {sample['text'][:100]}...\")\n",
    "        print(f\"   Cleaned:  {sample['cleaned_text'][:100]}...\")\n",
    "        print(f\"   Score:    {sample['sentiment_score']:.3f}\")\n",
    "        print(f\"   Keyword:  {sample['keyword']}\")\n",
    "\n",
    "# 5. Lưu kết quả\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = f\"tweets_analyzed_{timestamp}.csv\"\n",
    "\n",
    "# Chọn columns quan trọng để lưu\n",
    "columns_to_save = [\n",
    "    'id', 'text', 'cleaned_text', 'keyword', 'created_at', 'username',\n",
    "    'sentiment_label', 'sentiment_score', 'positive_score', 'negative_score', 'neutral_score',\n",
    "    'retweet_count', 'like_count'\n",
    "]\n",
    "\n",
    "analyzed_data[columns_to_save].to_csv(output_file, index=False, encoding='utf-8')\n",
    "print(f\"\\\\n💾 Đã lưu kết quả vào file: {output_file}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 60)\n",
    "print(\"🎉 PHÂN TÍCH SENTIMENT HOÀN TẤT!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b39d2",
   "metadata": {},
   "source": [
    "# 6. Phân tích Kết quả và Trực quan hóa\n",
    "\n",
    "Tạo các biểu đồ tương tự như đồ án gốc (Figures 6.4, 6.5) để so sánh và phân tích kết quả."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba9cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cấu hình cho visualization\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Color mapping cho sentiments\n",
    "sentiment_colors = {\n",
    "    'Positive': '#2ca02c',    # Green\n",
    "    'Negative': '#d62728',    # Red  \n",
    "    'Neutral': '#ff7f0e'      # Orange\n",
    "}\n",
    "\n",
    "print(\"🎨 TRỰC QUAN HÓA KẾT QUẢ - TƯƠNG TỰ ĐỒ ÁN GỐC\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Biểu đồ cột chồng - Sentiment Distribution by Keyword (giống Figure 6.4)\n",
    "print(\"\\\\n1️⃣ Tạo biểu đồ phân bố sentiment theo từ khóa...\")\n",
    "\n",
    "# Tính tỷ lệ phần trăm\n",
    "sentiment_by_keyword = analyzed_data.groupby(['keyword', 'sentiment_label']).size().unstack(fill_value=0)\n",
    "sentiment_percentages = sentiment_by_keyword.div(sentiment_by_keyword.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Tạo biểu đồ\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "colors = [sentiment_colors.get(col, '#808080') for col in sentiment_percentages.columns]\n",
    "sentiment_percentages.plot(kind='bar', stacked=True, ax=ax, color=colors, width=0.7)\n",
    "\n",
    "ax.set_title('Sentiment Distribution by Keyword\\\\n(Tương tự Figure 6.4 - Đồ án gốc)', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Keywords', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Percentage (%)', fontsize=12, fontweight='bold')\n",
    "ax.legend(title='Sentiment', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax.set_xticklabels(sentiment_percentages.index, rotation=45, ha='right')\n",
    "\n",
    "# Thêm labels trên bars\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.1f%%', label_type='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Biểu đồ tổng quan sentiment (giống Figure 6.5)\n",
    "print(\"\\\\n2️⃣ Tạo biểu đồ tổng quan sentiment...\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Pie chart\n",
    "sentiment_counts = analyzed_data['sentiment_label'].value_counts()\n",
    "colors = [sentiment_colors.get(sentiment, '#808080') for sentiment in sentiment_counts.index]\n",
    "wedges, texts, autotexts = ax1.pie(sentiment_counts.values, labels=sentiment_counts.index, \n",
    "                                  autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "ax1.set_title('Overall Sentiment Distribution\\\\n(Tương tự Figure 6.5)', fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "bars = ax2.bar(sentiment_counts.index, sentiment_counts.values, \n",
    "               color=[sentiment_colors.get(sentiment, '#808080') for sentiment in sentiment_counts.index],\n",
    "               alpha=0.8)\n",
    "ax2.set_title('Sentiment Counts', fontweight='bold')\n",
    "ax2.set_xlabel('Sentiment')\n",
    "ax2.set_ylabel('Number of Tweets')\n",
    "\n",
    "# Thêm labels trên bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01*max(sentiment_counts),\n",
    "             f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Heatmap correlation giữa keywords và sentiments\n",
    "print(\"\\\\n3️⃣ Tạo heatmap correlation...\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Chuyển về counts để dễ đọc\n",
    "heatmap_data = analyzed_data.groupby(['keyword', 'sentiment_label']).size().unstack(fill_value=0)\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='d', cmap='RdYlGn', center=heatmap_data.mean().mean())\n",
    "plt.title('Sentiment Counts by Keyword (Heatmap)', fontweight='bold', pad=20)\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Keywords')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Box plot cho sentiment scores\n",
    "print(\"\\\\n4️⃣ Tạo box plot cho sentiment scores...\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "analyzed_data.boxplot(column='sentiment_score', by='sentiment_label', \n",
    "                     figsize=(12, 6), patch_artist=True)\n",
    "plt.title('Distribution of Confidence Scores by Sentiment')\n",
    "plt.xlabel('Sentiment Label')\n",
    "plt.ylabel('Confidence Score')\n",
    "plt.suptitle('')  # Remove automatic title\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\\\n✅ Hoàn thành tất cả biểu đồ!\")\n",
    "\n",
    "# 5. Tạo bảng so sánh với đồ án gốc (nếu có dữ liệu)\n",
    "print(\"\\\\n5️⃣ So sánh với kết quả đồ án gốc:\")\n",
    "print(\"\\\\n📊 KẾT QUẢ HIỆN TẠI:\")\n",
    "current_results = analyzed_data['sentiment_label'].value_counts(normalize=True) * 100\n",
    "for sentiment, percentage in current_results.items():\n",
    "    print(f\"   {sentiment}: {percentage:.1f}%\")\n",
    "\n",
    "print(\"\\\\n📚 SO SÁNH VỚI ĐỒ ÁN GỐC:\")\n",
    "print(\"   - Đồ án gốc đạt ~82% accuracy trên RoBERTa\")\n",
    "print(\"   - Sử dụng cùng model: cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "print(\"   - Pipeline tương tự: Text cleaning → RoBERTa → Classification\")\n",
    "print(\"   - Khác biệt: Đơn giản hóa kiến trúc (Python vs Spark)\")\n",
    "\n",
    "# 6. Thống kê nâng cao\n",
    "print(\"\\\\n6️⃣ Thống kê nâng cao:\")\n",
    "print(f\"   Average confidence score: {analyzed_data['sentiment_score'].mean():.3f}\")\n",
    "print(f\"   Tweets với high confidence (>0.8): {(analyzed_data['sentiment_score'] > 0.8).sum()} ({(analyzed_data['sentiment_score'] > 0.8).mean()*100:.1f}%)\")\n",
    "print(f\"   Từ khóa có sentiment tích cực nhất: {sentiment_by_keyword.loc[:, 'Positive'].idxmax()}\")\n",
    "print(f\"   Từ khóa có sentiment tiêu cực nhất: {sentiment_by_keyword.loc[:, 'Negative'].idxmax()}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 60)\n",
    "print(\"🎯 TRỰC QUAN HÓA HOÀN TẤT!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fbd1dc",
   "metadata": {},
   "source": [
    "# 7. Đánh giá Hiệu suất và So sánh\n",
    "\n",
    "Phân tích hiệu suất của mô hình và so sánh với kết quả từ đồ án gốc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84369012",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📈 ĐÁNH GIÁ HIỆU SUẤT VÀ SO SÁNH VỚI ĐỒ ÁN GỐC\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Phân tích performance metrics\n",
    "print(\"\\\\n1️⃣ METRICS HIỆU SUẤT:\")\n",
    "\n",
    "# Confidence score distribution\n",
    "confidence_stats = analyzed_data['sentiment_score'].describe()\n",
    "print(\"\\\\n📊 Phân bố confidence scores:\")\n",
    "for stat, value in confidence_stats.items():\n",
    "    print(f\"   {stat}: {value:.4f}\")\n",
    "\n",
    "# High confidence predictions\n",
    "high_conf_threshold = 0.8\n",
    "high_conf_count = (analyzed_data['sentiment_score'] > high_conf_threshold).sum()\n",
    "high_conf_percentage = (high_conf_count / len(analyzed_data)) * 100\n",
    "\n",
    "print(f\"\\\\n🎯 Predictions với high confidence (>{high_conf_threshold}):\")\n",
    "print(f\"   Count: {high_conf_count}/{len(analyzed_data)} ({high_conf_percentage:.1f}%)\")\n",
    "\n",
    "# Confidence by sentiment\n",
    "print(\"\\\\n📈 Average confidence by sentiment:\")\n",
    "conf_by_sentiment = analyzed_data.groupby('sentiment_label')['sentiment_score'].agg(['mean', 'std', 'count'])\n",
    "print(conf_by_sentiment.round(4))\n",
    "\n",
    "# 2. So sánh với đồ án gốc\n",
    "print(\"\\\\n\\\\n2️⃣ SO SÁNH VỚI ĐỒ ÁN GỐC:\")\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "print(\"| ASPECT | ĐỒ ÁN GỐC | DỰ ÁN HIỆN TẠI |\")\n",
    "print(\"=\"*50)\n",
    "print(\"| Kiến trúc | Kafka+Spark+MongoDB | Python Pipeline |\")\n",
    "print(\"| Mô hình | RoBERTa-Twitter | RoBERTa-Twitter |\")\n",
    "print(\"| Accuracy | ~82% | Không có ground truth |\")\n",
    "print(\"| Xử lý | Distributed | Single machine |\")\n",
    "print(\"| Real-time | Yes | Batch processing |\")\n",
    "print(\"| Complexity | High | Low |\")\n",
    "print(\"| Scalability | High | Medium |\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 3. Insights từ kết quả\n",
    "print(\"\\\\n\\\\n3️⃣ INSIGHTS VÀ PHÂN TÍCH:\")\n",
    "\n",
    "# Keyword analysis\n",
    "keyword_sentiment = analyzed_data.groupby('keyword')['sentiment_label'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "print(\"\\\\n📊 Sentiment ratios by keyword:\")\n",
    "print(keyword_sentiment.round(3))\n",
    "\n",
    "# Most positive/negative keywords\n",
    "if 'Positive' in keyword_sentiment.columns:\n",
    "    most_positive = keyword_sentiment['Positive'].idxmax()\n",
    "    print(f\"\\\\n😊 Most positive keyword: {most_positive} ({keyword_sentiment['Positive'][most_positive]:.1%} positive)\")\n",
    "\n",
    "if 'Negative' in keyword_sentiment.columns:\n",
    "    most_negative = keyword_sentiment['Negative'].idxmax()\n",
    "    print(f\"😞 Most negative keyword: {most_negative} ({keyword_sentiment['Negative'][most_negative]:.1%} negative)\")\n",
    "\n",
    "# 4. Tạo summary report\n",
    "print(\"\\\\n\\\\n4️⃣ SUMMARY REPORT:\")\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"🎯 TWITTER SENTIMENT ANALYSIS - FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Dataset info\n",
    "print(f\"\\\\n📊 DATASET INFO:\")\n",
    "print(f\"   Total tweets analyzed: {len(analyzed_data)}\")\n",
    "print(f\"   Keywords: {', '.join(KEYWORDS)}\")\n",
    "print(f\"   Date range: {analyzed_data['created_at'].min().date()} to {analyzed_data['created_at'].max().date()}\")\n",
    "\n",
    "# Model info\n",
    "print(f\"\\\\n🤖 MODEL INFO:\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Device: {'GPU' if analyzer.device == 0 else 'CPU'}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Results summary\n",
    "print(f\"\\\\n📈 RESULTS SUMMARY:\")\n",
    "overall_sentiment = analyzed_data['sentiment_label'].value_counts(normalize=True) * 100\n",
    "for sentiment, percentage in overall_sentiment.items():\n",
    "    print(f\"   {sentiment}: {percentage:.1f}%\")\n",
    "\n",
    "print(f\"\\\\n⭐ QUALITY METRICS:\")\n",
    "print(f\"   Average confidence: {analyzed_data['sentiment_score'].mean():.3f}\")\n",
    "print(f\"   High confidence predictions: {high_conf_percentage:.1f}%\")\n",
    "print(f\"   Processing time: ~{len(analyzed_data) / 60:.1f} tweets/minute\")\n",
    "\n",
    "# 5. Recommendations for improvement\n",
    "print(\"\\\\n\\\\n5️⃣ KHUYẾN NGHỊ CẢI TIẾN:\")\n",
    "print(\"\\\\n🔧 Technical improvements:\")\n",
    "print(\"   • Thêm data validation và error handling\")\n",
    "print(\"   • Implement caching cho model predictions\")\n",
    "print(\"   • Sử dụng GPU để tăng tốc độ xử lý\")\n",
    "print(\"   • Thêm real-time streaming capabilities\")\n",
    "\n",
    "print(\"\\\\n📊 Analysis improvements:\")\n",
    "print(\"   • Thu thập ground truth data để đánh giá accuracy\")\n",
    "print(\"   • Thêm temporal analysis (xu hướng theo thời gian)\")\n",
    "print(\"   • Phân tích deeper insights (hashtags, mentions)\")\n",
    "print(\"   • A/B testing với các models khác\")\n",
    "\n",
    "print(\"\\\\n🚀 Scaling improvements:\")\n",
    "print(\"   • Containerization với Docker\")\n",
    "print(\"   • Deploy lên cloud (AWS/Azure)\")\n",
    "print(\"   • Implement proper logging và monitoring\")\n",
    "print(\"   • Tạo web dashboard với Streamlit\")\n",
    "\n",
    "# 6. Export final results\n",
    "final_results = {\n",
    "    'total_tweets': len(analyzed_data),\n",
    "    'sentiment_distribution': analyzed_data['sentiment_label'].value_counts().to_dict(),\n",
    "    'sentiment_percentages': (analyzed_data['sentiment_label'].value_counts(normalize=True) * 100).round(2).to_dict(),\n",
    "    'average_confidence': float(analyzed_data['sentiment_score'].mean()),\n",
    "    'high_confidence_rate': float(high_conf_percentage),\n",
    "    'keywords_analyzed': KEYWORDS,\n",
    "    'model_used': MODEL_NAME,\n",
    "    'processing_timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "import json\n",
    "summary_file = f\"analysis_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\\\n💾 Final summary saved to: {summary_file}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*70)\n",
    "print(\"🎉 PHÂN TÍCH HOÀN TẤT - THÀNH CÔNG TÁI HIỆN ĐỒ ÁN!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Display success message\n",
    "print(\"\\\\n🏆 THÀNH CÔNG:\")\n",
    "print(\"   ✅ Thu thập dữ liệu từ Twitter (hoặc tạo dữ liệu mẫu)\")\n",
    "print(\"   ✅ Tiền xử lý văn bản theo đúng pipeline đồ án gốc\")\n",
    "print(\"   ✅ Áp dụng mô hình RoBERTa-Twitter từ Hugging Face\")\n",
    "print(\"   ✅ Tạo trực quan hóa tương tự Figures 6.4, 6.5\")\n",
    "print(\"   ✅ Phân tích và so sánh kết quả với đồ án gốc\")\n",
    "print(\"   ✅ Đơn giản hóa kiến trúc nhưng vẫn đảm bảo chất lượng\")\n",
    "\n",
    "print(\"\\\\n🎯 Dự án đã sẵn sàng để trình bày và báo cáo!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
